{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78e5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline     \n",
    "## use `%matplotlib notebook` for interactive figures\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import sklearn\n",
    "import glob,sys\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import proplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3970a43f-5a8c-4183-96a7-409540c7d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1,'../')\n",
    "import preprocess\n",
    "p1=\"../../timeseries_csv/ts_wp/\"\n",
    "p2=\"../../../targets/\"\n",
    "\n",
    "ds47=preprocess._process_dataset(glob.glob(p1+'*2010OMAIS*')[0])\n",
    "tcwp47=ds47.values\n",
    "var_names=ds47.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab69a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from natsort import natsorted\n",
    "def depickle(loc=None):\n",
    "    output = []\n",
    "    with open(loc,'rb') as f:\n",
    "        output.append(pickle.load(f))\n",
    "    return output[0]\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def remove_strange_pred_aligned(TYPE='pmin',pred=None,truth=None):\n",
    "    truth=np.asarray(truth)\n",
    "    pred=np.asarray(pred)\n",
    "    truth = truth[(pred>0) & (pred<10000)]\n",
    "    pred = pred[(pred>0) & (pred<10000)]\n",
    "    return truth,pred\n",
    "\n",
    "def give_r2(filelocs='./WPAC/pcmci_2tau16/causalwpac_pmin_2tau16.obj.',\n",
    "            suffix=str('*12348'),pc_alpha=[0.1]):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    #for pc_alpha in tqdm([0.01,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]):\n",
    "    for pc_alpha in tqdm(pc_alpha):\n",
    "        #filelist = sorted(glob.glob('./combinedscript/pickleddata2/12348/*pmin*.obj.0.2.*.12348'))\n",
    "        filelistt =sorted(glob.glob(filelocs+str(pc_alpha)+suffix))[5::-1]+\\\n",
    "        sorted(glob.glob(filelocs+str(pc_alpha)+suffix))[6:]\n",
    "        files = [depickle(obj) for obj in filelistt]\n",
    "        temp_r2train = [r2_score(file['y']['train'],file['mlr'].predict(file['X']['train'])) for file in files]\n",
    "        temp_r2test = [r2_score(file['y']['valid']+file['y']['test'],\\\n",
    "                                file['mlr'].predict(file['X']['valid']+file['X']['test'])) for file in files]\n",
    "        temp = [np.asarray([list(obj) for obj in file['X']['train']]).shape[1] for file in files]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test.append(temp_r2test)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcmci = {'r2_train':store_r2train,'r2_test':store_r2test,'size':store_size}\n",
    "    del store_r2train,store_r2test,store_size\n",
    "    return storedict_pcmci\n",
    "\n",
    "def give_r2_pcstable(filelocs='./WPAC/pcstable_2tau16/newcausalwpac_2-16_pmin*',filelists=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    for pc_alpha in tqdm([0.2,0.1,0.05,0.01,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8]):\n",
    "        filelist = sorted(glob.glob(filelocs))\n",
    "        files = [depickle(obj) for obj in filelist]\n",
    "        temp_r2train = [r2_score(file['y']['train'],file['mlr'].predict(file['X']['train'])) for file in files]\n",
    "        temp_r2test = [r2_score(file['y']['valid']+file['y']['test'],\\\n",
    "                                file['mlr'].predict(file['X']['valid']+file['X']['test'])) for file in files]\n",
    "        temp = [np.asarray([list(obj) for obj in file['X']['train']]).shape[1] for file in files]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test.append(temp_r2test)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test':store_r2test,'size':store_size}\n",
    "    del store_r2train,store_r2test,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_r2_aligned(filelist=None,modelname='mlr'):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    for file in tqdm(files):\n",
    "        temp_r2train = r2_score(file['y']['train'],file[modelname].predict(file['X']['train']))\n",
    "        temp_r2test_old = r2_score(file['y']['valid']+file['y']['test'],file[modelname].predict(file['X']['valid']+file['X']['test']))\n",
    "        temp_r2test_new = r2_score(file['y']['newtest'],file[modelname].predict(file['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in file['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test_old':store_r2test_old,'r2_test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "def give_rmse_mae_aligned(filelist=None,TYPE='RMSE',modelname='mlr'):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    for file in files:\n",
    "        if TYPE=='RMSE':\n",
    "            temp_r2train = mean_squared_error(file['y']['train'],file[modelname].predict(file['X']['train']))\n",
    "            temp_r2test_old = mean_squared_error(file['y']['valid']+file['y']['test'],file[modelname].predict(file['X']['valid']+file['X']['test']))\n",
    "            temp_r2test_new = mean_squared_error(file['y']['newtest'],file[modelname].predict(file['X']['newtest']))\n",
    "        elif TYPE=='MAE':\n",
    "            temp_r2train = mean_absolute_error(file['y']['train'],file[modelname].predict(file['X']['train']))\n",
    "            temp_r2test_old = mean_absolute_error(file['y']['valid']+file['y']['test'],file[modelname].predict(file['X']['valid']+file['X']['test']))\n",
    "            temp_r2test_new = mean_absolute_error(file['y']['newtest'],file[modelname].predict(file['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in file['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'train':store_r2train,'test_old':store_r2test_old,'test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_r2_random(filelocs='./pickleddata_random/12348/randomwpall_2_16*pmin*'):\n",
    "    random_files = sorted(glob.glob(filelocs))\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    for pc_alpha in tqdm(range(len(random_files))):\n",
    "        #filelist = sorted(glob.glob('./combinedscript/pickleddata2/12348/*pmin*.obj.0.2.*.12348'))\n",
    "        files = [depickle(obj) for obj in random_files]\n",
    "        temp_r2train = [r2_score(file['y']['train'],file['mlr'].predict(file['X']['train'])) for file in files]\n",
    "        temp_r2test_old = [r2_score(file['y']['valid']+file['y']['test'],\\\n",
    "                                file['mlr'].predict(file['X']['valid']+file['X']['test'])) for file in files]\n",
    "        temp_r2test_new = [r2_score(file['y']['newtest'],file['mlr'].predict(file['X']['newtest'])) for file in files]\n",
    "        temp = [np.asarray([list(obj) for obj in file['X']['train']]).shape[1] for file in files]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        storenatsort_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test_old':store_r2test_old,'r2_test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_random\n",
    "\n",
    "def give_r2_lag(filelocs='./pickleddata_lag/12348/lagwpacall8_24*pmin*'):\n",
    "    fileslist = glob.glob(filelocs)[5::-1]+glob.glob(filelocs)[6:]\n",
    "    files = [depickle(obj) for obj in fileslist]\n",
    "    temp_r2train = [r2_score(file['y']['train'],file['mlr'].predict(file['X']['train'])) for file in files]\n",
    "    temp_r2test_old = [r2_score(file['y']['valid']+file['y']['test'],file['mlr'].predict(file['X']['valid']+file['X']['test'])) for file in files]\n",
    "    temp_r2test_new = [r2_score(file['y']['newtest'],file['mlr'].predict(file['X']['newtest'])) for file in files]\n",
    "    temp = [np.asarray([list(obj) for obj in file['X']['train']]).shape[1] for file in files]\n",
    "    storedict_lag = {'r2_train':temp_r2train,'r2_test_old':temp_r2test_old,'r2_test_new':temp_r2test_new,'size':temp}\n",
    "    del temp_r2train,temp_r2test_old,temp_r2test_new\n",
    "    return storedict_lag\n",
    "\n",
    "def give_r2_anatsortligned_rf(filelist=None,alt_filelist=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    fileXy = [depickle(obj) for obj in alt_filelist]\n",
    "    for ind,file in enumerate(files):\n",
    "        temp_r2train = r2_score(fileXy[ind]['y']['train'],file.predict(fileXy[ind]['X']['train']))\n",
    "        temp_r2test_old = r2_score(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file.predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "        temp_r2test_new = r2_score(fileXy[ind]['y']['newtest'],file.predict(fileXy[ind]['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in fileXy[ind]['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test_old':store_r2test_old,'r2_test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_rmse_mae_aligned_rf(filelist=None,TYPE='RMSE',alt_filelist=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    fileXy = [depickle(obj) for obj in alt_filelist]\n",
    "    for ind,file in tqdm(enumerate(files)):\n",
    "        if TYPE=='RMSE':\n",
    "            temp_r2train = mean_squared_error(fileXy[ind]['y']['train'],file.predict(fileXy[ind]['X']['train']))\n",
    "            temp_r2test_old = mean_squared_error(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file.predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "            temp_r2test_new = mean_squared_error(fileXy[ind]['y']['newtest'],file.predict(fileXy[ind]['X']['newtest']))\n",
    "        elif TYPE=='MAE':\n",
    "            temp_r2train = mean_absolute_error(fileXy[ind]['y']['train'],file.predict(fileXy[ind]['X']['train']))\n",
    "            temp_r2test_old = mean_absolute_error(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file.predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "            temp_r2test_new = mean_absolute_error(fileXy[ind]['y']['newtest'],file.predict(fileXy[ind]['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in fileXy[ind]['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'train':store_r2train,'test_old':store_r2test_old,'test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_rmse_mae_aligned_rf2(filelist=None,TYPE='RMSE',alt_filelist=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    fileXy = [depickle(obj) for obj in alt_filelist]\n",
    "    for ind,file in tqdm(enumerate(files)):\n",
    "        if TYPE=='RMSE':\n",
    "            temp_r2train = mean_squared_error(fileXy[ind]['y']['train'],file['model'].predict(fileXy[ind]['X']['train']))\n",
    "            temp_r2test_old = mean_squared_error(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file['model'].predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "            temp_r2test_new = mean_squared_error(fileXy[ind]['y']['newtest'],file['model'].predict(fileXy[ind]['X']['newtest']))\n",
    "        elif TYPE=='MAE':\n",
    "            temp_r2train = mean_absolute_error(fileXy[ind]['y']['train'],file['model'].predict(fileXy[ind]['X']['train']))\n",
    "            temp_r2test_old = mean_absolute_error(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file['model'].predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "            temp_r2test_new = mean_absolute_error(fileXy[ind]['y']['newtest'],file['model'].predict(fileXy[ind]['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in fileXy[ind]['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'train':store_r2train,'test_old':store_r2test_old,'test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_r2_aligned_rf(filelist=None,alt_filelist=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    fileXy = [depickle(obj) for obj in alt_filelist]\n",
    "    for ind,file in tqdm(enumerate(files)):\n",
    "        temp_r2train = r2_score(fileXy[ind]['y']['train'],file.predict(fileXy[ind]['X']['train']))\n",
    "        temp_r2test_old = r2_score(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file.predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "        temp_r2test_new = r2_score(fileXy[ind]['y']['newtest'],file.predict(fileXy[ind]['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in fileXy[ind]['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test_old':store_r2test_old,'r2_test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable\n",
    "\n",
    "def give_r2_aligned_rf2(filelist=None,alt_filelist=None):\n",
    "    store_r2train,store_r2test_old,store_r2test_new,store_size = [],[],[],[]\n",
    "    files = [depickle(obj) for obj in filelist]\n",
    "    fileXy = [depickle(obj) for obj in alt_filelist]\n",
    "    for ind,file in tqdm(enumerate(files)):\n",
    "        temp_r2train = r2_score(fileXy[ind]['y']['train'],file['model'].predict(fileXy[ind]['X']['train']))\n",
    "        temp_r2test_old = r2_score(fileXy[ind]['y']['valid']+fileXy[ind]['y']['test'],file['model'].predict(fileXy[ind]['X']['valid']+fileXy[ind]['X']['test']))\n",
    "        temp_r2test_new = r2_score(fileXy[ind]['y']['newtest'],file['model'].predict(fileXy[ind]['X']['newtest']))\n",
    "        temp = np.asarray([list(obj) for obj in fileXy[ind]['X']['train']]).shape[1]\n",
    "        store_r2train.append(temp_r2train)\n",
    "        store_r2test_old.append(temp_r2test_old)\n",
    "        store_r2test_new.append(temp_r2test_new)\n",
    "        store_size.append(temp)\n",
    "    storedict_pcstable = {'r2_train':store_r2train,'r2_test_old':store_r2test_old,'r2_test_new':store_r2test_new,'size':store_size}\n",
    "    del store_r2train,store_r2test_old,store_r2test_new,store_size\n",
    "    return storedict_pcstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d957b88-6a8c-45b9-bee5-c658ad81b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa934674-ef69-47be-9799-5bb6ed0c5e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921c141cfcf14aadbff8c3895a0aa307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################################################################################################################\n",
    "# Read files (PC1)\n",
    "listtt = (sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac*aligned_8-24*v10*')))\n",
    "# Wrong order to correct...\n",
    "aligned2_16_list = [listtt[-1],listtt[-4],listtt[-2],listtt[-5],listtt[-3],listtt[-6]]\n",
    "for obj in listtt[:-6]:\n",
    "    aligned2_16_list.append(obj)\n",
    "#####################################################################################################################################    \n",
    "#pcmci_wpac_2tau16 = give_r2()\n",
    "pcstable_wpac_2tau16 = give_r2_aligned(aligned2_16_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e6625b-659f-412e-ac71-46731e2010dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_wpac_2tau16 = give_rmse_mae_aligned(aligned2_16_list,'RMSE')\n",
    "mae_wpac_2tau16 = give_rmse_mae_aligned(aligned2_16_list,'MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35c04b8-b171-46c7-aaaa-7096c91213ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'r2_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r2_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(pcstable_wpac_2tau16[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2_train\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mpcstable_wpac_2tau16\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:aligned2_16_list})\n\u001b[1;32m      2\u001b[0m rmse_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(rmse_wpac_2tau16[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(rmse_wpac_2tau16[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:aligned2_16_list})\n\u001b[1;32m      3\u001b[0m mae_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(mae_wpac_2tau16[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39masarray(mae_wpac_2tau16[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:aligned2_16_list})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'r2_test'"
     ]
    }
   ],
   "source": [
    "r2_df = pd.DataFrame({'train':np.asarray(pcstable_wpac_2tau16['r2_train']).flatten(),'test':np.asarray(pcstable_wpac_2tau16['r2_test']).flatten(),'name':aligned2_16_list})\n",
    "rmse_df = pd.DataFrame({'train':np.asarray(rmse_wpac_2tau16['train']).flatten(),'test':np.asarray(rmse_wpac_2tau16['test']).flatten(),'name':aligned2_16_list})\n",
    "mae_df = pd.DataFrame({'train':np.asarray(mae_wpac_2tau16['train']).flatten(),'test':np.asarray(mae_wpac_2tau16['test']).flatten(),'name':aligned2_16_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4de9ee-395f-4403-b474-3195430e539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr2 = np.asarray(pcstable_wpac_2tau16['r2_test']).max()\n",
    "index_r2_best,index_r2_1percent = r2_df['test'].idxmax(),r2_df[r2_df['test']>maxr2-maxr2*0.01].index\n",
    "index_r2_best\n",
    "#import rfreg_funcs\n",
    "#rfreg_funcs.save_to_pickle('./tmp/multimodelnames_fig4.pkl',list(r2_df.iloc[index_r2_1percent]['name'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c62ec-a4cb-402c-9592-2f32f4717abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfreg_funcs\n",
    "lag_wpac_2tau16 = give_r2_lag('./pickleddata_lag/12348/lagwpacall8_24*v10*')\n",
    "storelag = {'filename':glob.glob('./pickleddata_lag/12348/lagwpacall8_24*v10*')[5::-1]+glob.glob('./pickleddata_lag/12348/lagwpacall8_24*v10*')[6:],\n",
    "            'r2_test': lag_wpac_2tau16['r2_test'],'r2_train':lag_wpac_2tau16['r2_train']}\n",
    "print(index_r2_best)\n",
    "#rfreg_funcs.save_to_pickle('./tmp/multimodelnames_fig4_lag.pkl',list(df_lag.iloc[index_r2_1percent]['filename'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e94dc-b154-4935-bf44-b4279d343a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Feature Selection MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49873145-94f4-4dcf-8197-237569389fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_performance_r2(listt=None,var='R2',TYPE='MLR',listtt_mlr=None,modelname='mlr'):\n",
    "    if var=='R2':\n",
    "        if TYPE=='MLR':\n",
    "            files = give_r2_aligned(listt,modelname)\n",
    "        elif TYPE=='RF':\n",
    "            from natsort import natsorted\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/rf/*pmin*'))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list = [listt[-1],listt[-4],listt[-2],listt[-5],listt[-3],listt[-6]]\n",
    "            for obj in listt[:-6]:\n",
    "                aligned2_16_list.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt_mlr = (sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac_aligned_8-24_pmin*')))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list_mlr = [listtt_mlr[-1],listtt_mlr[-4],listtt_mlr[-2],listtt_mlr[-5],listtt_mlr[-3],listtt_mlr[-6]]\n",
    "            for obj in listtt_mlr[:-6]:\n",
    "                aligned2_16_list_mlr.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            files = give_r2_aligned_rf(aligned2_16_list,aligned2_16_list_mlr)\n",
    "            #try:\n",
    "            #    print('hi')\n",
    "            #    files = give_r2_aligned_rf(aligned2_16_list,aligned2_16_list_mlr)\n",
    "            #except:\n",
    "            #    print('hihi')\n",
    "            #    files = give_r2_aligned_rf2(aligned2_16_list,aligned2_16_list_mlr)\n",
    "\n",
    "        storelag = {'filename':listt,'r2_test_new': files['r2_test_new'],'r2_train':files['r2_train'],'r2_valid':files['r2_test_old']}\n",
    "        r2_df = pd.DataFrame({'train':np.asarray(storelag['r2_train']).flatten(),\\\n",
    "                              'test':np.asarray(storelag['r2_test_new']).flatten(),\\\n",
    "                              'valid':np.asarray(storelag['r2_valid']).flatten(),\\\n",
    "                              'name':storelag['filename']})\n",
    "        df_lag = pd.DataFrame(storelag)\n",
    "        maxr2 = np.asarray(df_lag['r2_valid']).max()\n",
    "        index_r2_best,index_r2_1percent = df_lag['r2_valid'].idxmax(),df_lag[df_lag['r2_valid']>maxr2-maxr2*0.10].index\n",
    "        return storelag,float(r2_df.iloc[index_r2_best]['train']), float(r2_df.iloc[index_r2_best]['valid']), float(r2_df.iloc[index_r2_best]['test']),\\\n",
    "    float(np.asarray(files['size'])[index_r2_best])\n",
    "\n",
    "def output_performance_mae_rmse(listt=None,var='RMSE',TYPE='MLR',listtt_mlr=None,modelname='mlr',r2dict=None):\n",
    "    if var=='MAE':\n",
    "        if TYPE=='MLR':\n",
    "            files = give_rmse_mae_aligned(listt,var,modelname)\n",
    "        elif TYPE=='RF':\n",
    "            from natsort import natsorted\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/rf/*pmin*'))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list = [listt[-1],listt[-4],listt[-2],listt[-5],listt[-3],listt[-6]]\n",
    "            for obj in listt[:-6]:\n",
    "                aligned2_16_list.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt_mlr = (sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac_aligned_8-24_pmin*')))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list_mlr = [listtt_mlr[-1],listtt_mlr[-4],listtt_mlr[-2],listtt_mlr[-5],listtt_mlr[-3],listtt_mlr[-6]]\n",
    "            for obj in listtt_mlr[:-6]:\n",
    "                aligned2_16_list_mlr.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            files = give_rmse_mae_aligned_rf(aligned2_16_list,var,aligned2_16_list_mlr)\n",
    "            #try:\n",
    "            #    files = give_rmse_mae_aligned_rf(aligned2_16_list,var,aligned2_16_list_mlr)\n",
    "            #except:\n",
    "            #    files = give_rmse_mae_aligned_rf2(aligned2_16_list,var,aligned2_16_list_mlr)\n",
    "                \n",
    "        r2_df = pd.DataFrame({'r2_train':np.asarray(r2dict['r2_train']).flatten(),'r2_valid':np.asarray(r2dict['r2_valid']).flatten(),\\\n",
    "                              'r2_test':np.asarray(r2dict['r2_test_new']).flatten(),'name':listt})\n",
    "        mae_df = pd.DataFrame({'train':np.asarray(files['train']).flatten(),'valid':np.asarray(files['test_old']).flatten(),\\\n",
    "                               'test':np.asarray(files['test_new']).flatten(),'name':listt})\n",
    "        df_lag = pd.DataFrame(r2_df)\n",
    "        maxr2 = np.asarray(df_lag['r2_valid']).max()\n",
    "        index_r2_best = df_lag['r2_valid'].idxmax()\n",
    "        return float(mae_df.iloc[index_r2_best]['train']),float(mae_df.iloc[index_r2_best]['valid']),\\\n",
    "    float(mae_df.iloc[index_r2_best]['test']),float(np.asarray(files['size'])[index_r2_best])\n",
    "    \n",
    "    elif var=='RMSE':\n",
    "        if TYPE=='MLR':\n",
    "            files = give_rmse_mae_aligned(listt,var,modelname)\n",
    "        elif TYPE=='RF':\n",
    "            from natsort import natsorted\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/rf/*pmin*'))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list = [listt[-1],listt[-4],listt[-2],listt[-5],listt[-3],listt[-6]]\n",
    "            for obj in listt[:-6]:\n",
    "                aligned2_16_list.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            # Read files (PC1)\n",
    "            #listtt_mlr = (sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac_aligned_8-24_pmin*')))\n",
    "            # Wrong order to correct...\n",
    "            aligned2_16_list_mlr = [listtt_mlr[-1],listtt_mlr[-4],listtt_mlr[-2],listtt_mlr[-5],listtt_mlr[-3],listtt_mlr[-6]]\n",
    "            for obj in listtt_mlr[:-6]:\n",
    "                aligned2_16_list_mlr.append(obj)\n",
    "            #####################################################################################################################################\n",
    "            files = give_rmse_mae_aligned_rf(aligned2_16_list,var,aligned2_16_list_mlr)\n",
    "        r2_df = pd.DataFrame({'r2_train':np.asarray(r2dict['r2_train']).flatten(),'r2_valid':np.asarray(r2dict['r2_valid']).flatten(),\\\n",
    "                              'r2_test':np.asarray(r2dict['r2_test_new']).flatten(),'name':listt})\n",
    "        rmse_df = pd.DataFrame({'train':np.asarray(files['train']).flatten(),'valid':np.asarray(files['test_old']).flatten(),\\\n",
    "                               'test':np.asarray(files['test_new']).flatten(),'name':listt})\n",
    "        df_lag = pd.DataFrame(r2_df)\n",
    "        maxr2 = np.asarray(df_lag['r2_valid']).max()\n",
    "        index_r2_best = df_lag['r2_valid'].idxmax()\n",
    "        return float(rmse_df.iloc[index_r2_best]['train']),float(rmse_df.iloc[index_r2_best]['valid']),\\\n",
    "    float(rmse_df.iloc[index_r2_best]['test']),float(np.asarray(files['size'])[index_r2_best])\n",
    "    \n",
    "def storeRMSE(path_prefix=None,TYPE='MLR',storename='mlr',alt_fileprefix=None,natsortted='No',modelstart=0):\n",
    "    storeRMSE = []\n",
    "    for target in ['pmin','v10','precip']:\n",
    "        #filelist = glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*'+str(target)+'.obj.*')\n",
    "        \n",
    "        if natsortted=='Yes':\n",
    "            listtt_lag = natsorted(glob.glob(path_prefix+str(target)+'.obj.*'))[int(modelstart):]\n",
    "        elif natsortted=='No':\n",
    "            listtt_lag = sorted(glob.glob(path_prefix+str(target)+'.obj.*'))[int(modelstart):]\n",
    "            \n",
    "        if TYPE=='MLR':\n",
    "            storer2,_,_,_,_ = output_performance_r2(listtt_lag,'R2',TYPE,None,storename)\n",
    "            train,valid,test,_ = output_performance_mae_rmse(listtt_lag,'RMSE',TYPE,None,storename,storer2)\n",
    "        elif TYPE=='RF':\n",
    "            if natsortted=='Yes':\n",
    "                alt_filelist = natsorted(glob.glob(alt_fileprefix+str(target)+'*'))[int(modelstart):]\n",
    "            elif natsortted=='No':\n",
    "                alt_filelist = sorted(glob.glob(alt_fileprefix+str(target)+'*'))[int(modelstart):]\n",
    "            storer2,_,_,_,_ = output_performance_r2(listtt_lag,'R2',TYPE,alt_filelist,storename)\n",
    "            train,valid,test,_ = output_performance_mae_rmse(listtt_lag,'RMSE',TYPE,alt_filelist,storename,storer2)    \n",
    "        \n",
    "        if target=='precip':\n",
    "            storeRMSE.append([train/1000,valid/1000,test/1000])\n",
    "        else:\n",
    "            storeRMSE.append([train,valid,test])\n",
    "    return [np.round(obj,decimals=2) for obj in np.ravel(np.asarray(storeRMSE),'F')]\n",
    "\n",
    "def storeMAE(path_prefix=None,TYPE='MLR',storename='mlr',alt_fileprefix=None,natsortted='No',modelstart=0):\n",
    "    storeMAE = []\n",
    "    for target in ['pmin','v10','precip']:\n",
    "        #filelist = glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*'+str(target)+'.obj.*')\n",
    "        if natsortted=='Yes':\n",
    "            listtt_lag = natsorted(glob.glob(path_prefix+str(target)+'.obj.*'))[int(modelstart):]\n",
    "        elif natsortted=='No':\n",
    "            listtt_lag = sorted(glob.glob(path_prefix+str(target)+'.obj.*'))[int(modelstart):]\n",
    "            \n",
    "        if TYPE=='MLR':\n",
    "            storer2,_,_,_,_ = output_performance_r2(listtt_lag,'R2',TYPE,None,storename)\n",
    "            train,valid,test,_ = output_performance_mae_rmse(listtt_lag,'MAE',TYPE,None,storename,storer2)\n",
    "        elif TYPE=='RF':\n",
    "            if natsortted=='Yes':\n",
    "                alt_filelist = natsorted(glob.glob(alt_fileprefix+str(target)+'*'))[int(modelstart):]\n",
    "            elif natsortted=='No':\n",
    "                alt_filelist = sorted(glob.glob(alt_fileprefix+str(target)+'*'))[int(modelstart):]\n",
    "            storer2,_,_,_,_ = output_performance_r2(listtt_lag,'R2',TYPE,alt_filelist,storename)\n",
    "            train,valid,test,_ = output_performance_mae_rmse(listtt_lag,'MAE',TYPE,alt_filelist,storename,storer2)   \n",
    "        if target=='precip':\n",
    "            storeMAE.append([train/1000,valid/1000,test/1000])\n",
    "        else:\n",
    "            storeMAE.append([train,valid,test])\n",
    "    return [np.round(obj,decimals=2) for obj in np.ravel(np.asarray(storeMAE),'F')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9467f045-91fd-47b1-9665-95136a5e0a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966976c39c3f49079b17cfb29e8d64b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random (start from 9)\n",
    "listtt = (natsorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random_rf/randomrf_8-24*v10.obj.*')))[9:]\n",
    "listtt_mlr = (natsorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random/12348/randomwp*8_24_v10*')))[9:]\n",
    "storer2,train_r2,valid_r2,test_r2,size = output_performance_r2(listtt,'R2','RF',listtt_mlr,'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdff33f-62d6-49ef-bb64-2537208f3780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9292547666305121, 0.7679355368775268, 0.765194226976501, 770.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_r2,valid_r2,test_r2,size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eabda8-211b-4774-9c39-73ee423ef4ca",
   "metadata": {},
   "source": [
    "## Creating Array for MAE - noncausal MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f691dc9-9825-496c-a661-9c605beeea08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1501488782.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    All=/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/noncausal/12348/*precip*\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "All=/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/noncausal/12348/*precip*\n",
    "lagged=/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*precip.obj.*\n",
    "random=/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random/12348/random*8_24_precip.obj.*\n",
    "xai=/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/rf_feature/12348/rfredall8_24_precip.obj.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c8ff0b7-033a-42f8-9c8b-e7ff66dbe4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodl = []\n",
    "for target in ['pmin','v10','precip']:\n",
    "    model = depickle(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/noncausal/12348/*'+str(target)+'*')[0])\n",
    "    if target=='precip':\n",
    "        allmodl.append([mean_squared_error(model['y']['train'],model['mlr'].predict(model['X']['train']))/1000,\\\n",
    "                        mean_squared_error(model['y']['valid']+model['y']['test'],model['mlr'].predict(model['X']['valid']+model['X']['test']))/1000,\\\n",
    "                        mean_squared_error(model['y']['newtest'],model['mlr'].predict(model['X']['newtest']))/1000])\n",
    "    else:\n",
    "        allmodl.append([mean_squared_error(model['y']['train'],model['mlr'].predict(model['X']['train'])),\\\n",
    "                        mean_squared_error(model['y']['valid']+model['y']['test'],model['mlr'].predict(model['X']['valid']+model['X']['test'])),\\\n",
    "                        mean_squared_error(model['y']['newtest'],model['mlr'].predict(model['X']['newtest']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de25f23-0ed9-4ed5-a996-0160f9c8dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c135fe82a342c0ab4d2e5032d15b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c543e90e347428e81c7aa7d51af4050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8db6f8c7144ff90c59cdfa0d76c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddad88837ab140aea12b65bce1bc93a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b019a6205c4325a20c06976d5e8c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f5519aabab47aa86327e7dfa3e3ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9063fac6597440afad7c9e5bb7db0143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f8df30e4284d43b08954551a513cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53821d2a848a44abb0cb320003f908bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "laglist = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*')\n",
    "randomlist = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random/12348/random*8_24*')\n",
    "xailist = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/rf_feature/12348/rfregall8_24*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd15234a-b8b0-414d-9d0b-74bcceedbc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305b4a36743447609d51ed3943afbf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc7f3bc070f4675944e31186053cc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5773472eee8f4cddb35ca54cfe563d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46ff727fde54c93804ebde0ce908134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0c0c4c0277460aa2a27bc290694ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e84a02e24b4f50ade09e426a1416b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12fc8b519c4b6f8b3e051c9a34b8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9747a1e75a94217b0b1c8acfa1745cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a112c02f3b94cd38afecc582fbf5b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlr_fileprefix = '/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac*8-24_*'\n",
    "causal_rf = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/rf/rfmodels_8-24_*','RF','model',mlr_fileprefix,'No',0)\n",
    "causal_mlr = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/parallel_tigramite/newdata/pcstablewpac*8-24_*','MLR','mlr',None,'Yes',0)\n",
    "#lstm = storeRMSE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/best_lstm/best_model3*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ceeb728-fdbb-4b09-b2a1-bea478e717d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfreg_funcs.save_to_pickle('./store/rmse_mae/storestuff_causalmlr_mae.pkl',causal_mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41672e1a-439a-4dab-9522-48fad58e35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfreg_funcs,torch\n",
    "import torch\n",
    "from torch import nn\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                hidden_units,\n",
    "                dropout_rates):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.units = hidden_units\n",
    "        self.drates = dropout_rates\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(input_size = input_size,\n",
    "                                 hidden_size = self.units,\n",
    "                                 num_layers = 1,\n",
    "                                 bias = True,\n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p = self.drates)\n",
    "        \n",
    "        self.dense_layer = nn.Linear(in_features = self.units, \n",
    "                                     out_features = self.units)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features = self.units,\n",
    "                                      out_features=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output, (h_n, c_n) = self.lstm_layer(X)\n",
    "        \n",
    "        hidden_state = self.dropout_layer(h_n[0])\n",
    "        #print(f'1st Hidden {hidden_state.shape}')\n",
    "        hidden_state2 = torch.tanh(self.dense_layer(hidden_state))\n",
    "        #print(f'2nd Hidden {hidden_state2.shape}')\n",
    "        p_hat = torch.flatten(self.output_layer((hidden_state2)))\n",
    "        \n",
    "        return p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9d90974-1383-4e5f-be86-9130000a0567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm_layer): LSTM(234, 69, batch_first=True)\n",
       "  (dropout_layer): Dropout(p=0, inplace=False)\n",
       "  (dense_layer): Linear(in_features=69, out_features=69, bias=True)\n",
       "  (output_layer): Linear(in_features=69, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/best_lstm/best_model3*')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415533f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5ff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bc37c-5757-443e-8537-a3109c5aec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noncausal_rf='/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/rf_feature/12348/rfregall8_24*'\n",
    "rf_lagged='/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag_rf/lagrf_8-24*'\n",
    "rf_random='/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random_rf/randomrf_8-24*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "666b9c1a-e85e-4957-87eb-eb24ed4e7d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68c1a3aa8cb47479d23e5e89d6a3210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d46cbafe774acd9d612d3eb3a9cb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0d5fb84bcb4632b2d2e53a5c08d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.04, 1.12, 0.14, 3.78, 1.95, 0.22, 3.41, 1.85, 0.23]\n"
     ]
    }
   ],
   "source": [
    "# Lag uses 'MLR' because we saved the X,y\n",
    "mlr_fileprefix = '/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*'\n",
    "rf_lagged = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag_rf/lagrf_8-24*',\n",
    "                      'MLR','model',mlr_fileprefix,'Yes',0)\n",
    "print(rf_lagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce0c075-21d4-46d2-878a-60ed0180f2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7aa689996e4af6847fb91451699305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96906a44a8974dae88e1bf1a1cd62ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdef03f6d0dd4f19bb61afdbe54e4eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca7f8bfbe7146f1afef5734d842d779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc12b0726bf46f9ab457a136d1c5c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e703dc64534f87907a854f24c21d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.07, 1.12, 0.12, 4.43, 2.2, 0.25, 3.94, 2.1, 0.25]\n"
     ]
    }
   ],
   "source": [
    "mlr_fileprefix = '/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random/12348/randomwp*8_24*'\n",
    "rf_random = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random_rf/randomrf_8-24*',\n",
    "                      'RF','model',mlr_fileprefix,'Yes',9)\n",
    "print(rf_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "165ac274-28a4-49f2-a5fe-aa09fa8786dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_lines(prefix=None,numbers=None):\n",
    "    for obj in [np.round(obj, decimals=2) for obj in list(numbers)]:\n",
    "        tofill = str(obj)+\" & \"\n",
    "        prefix+=tofill\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ef30efc-ff26-4d9a-b8d2-496b9e1ce181",
   "metadata": {},
   "outputs": [],
   "source": [
    "alllist = [np.round(obj,decimals=2) for obj in np.ravel(np.asarray(allmodl),'F')]\n",
    "storestuff = np.vstack([np.asarray(obj) for obj in [alllist,laglist,randomlist,xailist]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6af4a3f0-683a-486e-97ae-5852ee3e2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfreg_funcs\n",
    "rfreg_funcs.save_to_pickle('./store/rmse_mae/storestuff_noncausalrf_lagged_rmse.pkl',rf_lagged)\n",
    "rfreg_funcs.save_to_pickle('./store/rmse_mae/storestuff_noncausalrf_random_rmse.pkl',rf_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5d34d39d-78d4-4487-a786-28d612feffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table printed to ./testtable.txt\n"
     ]
    }
   ],
   "source": [
    "path = './testtable_rmse_noncausal_mlr.txt'\n",
    "f= open(path,\"w+\")\n",
    "f.write(\"\\\\begin{table}\\n\")\n",
    "f.write(\"\\\\begin{centering}\\n\")\n",
    "f.write(\"\\\\begin{tabular}{cc|c|c|c|c|c|c|c|c|c|c}\\n\")\n",
    "f.write(\"\\\\multicolumn{2}{c}{ML Models} & \\multicolumn{3}{c}{Training} & \\multicolumn{3}{c}{Validation} & \\multicolumn{3}{c|}{Test } & \\\\tabularnewline\")\n",
    "f.write(\"\\\\hline\\n\")\n",
    "\n",
    "sentence1 = \"\\\\multicolumn{2}{c|}{\\\\textbf{Target}} & Pmin (hPa)\\\\textcolor{purple}{{} } & V10 (m/s) & Precip (km\\\\textasciicircum 3) & Pmin (hPa)\\\\textcolor{purple}{{} } & V10 (m/s) & \"+\\\n",
    "\"Precip (km\\\\textasciicircum 3) & Pmin (hPa)\\\\textcolor{purple}{{} } & V10 (m/s) & Precip (km\\\\textasciicircum 3) & \\\\tabularnewline \"+\\\n",
    "\"\\cline{3-12} \\cline{4-12} \\cline{5-12} \\cline{6-12} \\cline{7-12} \\cline{8-12} \\cline{9-12} \\cline{10-12} \\cline{11-12} \\cline{12-12}\\n\"\n",
    "f.write(sentence1)\n",
    "f.write(handle_lines('\\\\multicolumn{2}{c|}{\\\\textbf{Causal RF}} & ' ,storestuff[0,:])+\"\\\\tabularnewline\\n\")\n",
    "f.write(handle_lines('\\\\multirow{4}{*}{\\\\textbf{Non-causal MLR}} & All & ',storestuff[0,:])+\"\\\\tabularnewline\\n\")\n",
    "f.write(handle_lines('\\cline{2-12} \\cline{3-12} \\cline{4-12} \\cline{5-12} \\cline{6-12} \\cline{7-12} \\cline{8-12} \\cline{9-12} \\cline{10-12} \\cline{11-12} \\cline{12-12} & Lagged & ',\n",
    "                     storestuff[1,:])+\"\\\\tabularnewline\\n\")\n",
    "f.write(handle_lines('\\cline{2-12} \\cline{3-12} \\cline{4-12} \\cline{5-12} \\cline{6-12} \\cline{7-12} \\cline{8-12} \\cline{9-12} \\cline{10-12} \\cline{11-12} \\cline{12-12} & Random & ',\n",
    "                     storestuff[2,:])+\"\\\\tabularnewline\\n\")\n",
    "f.write(handle_lines('\\cline{2-12} \\cline{3-12} \\cline{4-12} \\cline{5-12} \\cline{6-12} \\cline{7-12} \\cline{8-12} \\cline{9-12} \\cline{10-12} \\cline{11-12} \\cline{12-12} & XAI & ',\n",
    "                     storestuff[3,:])+\"\\\\tabularnewline\\n\")\n",
    "f.write(\"\\\\hline\\n\")\n",
    "f.write(\"\\\\end{tabular}\\n\")\n",
    "f.write(\"\\\\par\\\\end{centering}\\n\")\n",
    "f.write(\"\\\\end{table}\")\n",
    "f.close()\n",
    "print('Table printed to',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b8fcb563-1057-4bb9-bb85-f09461e164fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1e536cd7394945a06c02c08004d116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e5356629234ef19d60b887d6ceda6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1e8e82c2114acb83a838ec9ac3dc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773dbb1e6aa84a2ea222e7e5352dc43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08e79131fa8419bb128e6a9a45c7218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cab71771eb49e688055b05cd51d431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920026de9c8e41e1bdda1fe12d467d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52b23f48382407395c88679d490147e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cba6538ecb642dc9e1b51a2bd61478b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "laglistMAE = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/lag/12348/lagwpac*8_24*')\n",
    "randomlistMAE = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/random/12348/random*8_24*')\n",
    "xailistMAE = storeMAE('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/rf_feature/12348/rfregall8_24*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9f6ef4a5-9c03-41d8-9bef-4f9ab4456ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodlMAE = []\n",
    "for target in ['pmin','v10','precip']:\n",
    "    model = depickle(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/causal_notebooks/climateinformatics_revision1/store/noncausal/12348/*'+str(target)+'*')[0])\n",
    "    if target=='precip':\n",
    "        allmodlMAE.append([mean_absolute_error(model['y']['train'],model['mlr'].predict(model['X']['train']))/1000,\\\n",
    "                        mean_absolute_error(model['y']['valid']+model['y']['test'],model['mlr'].predict(model['X']['valid']+model['X']['test']))/1000,\\\n",
    "                        mean_absolute_error(model['y']['newtest'],model['mlr'].predict(model['X']['newtest']))/1000])\n",
    "    else:\n",
    "        allmodlMAE.append([mean_absolute_error(model['y']['train'],model['mlr'].predict(model['X']['train'])),\\\n",
    "                        mean_absolute_error(model['y']['valid']+model['y']['test'],model['mlr'].predict(model['X']['valid']+model['X']['test'])),\\\n",
    "                        mean_absolute_error(model['y']['newtest'],model['mlr'].predict(model['X']['newtest']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a5e15a35-68bf-48ce-bc48-3735763e6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "alllistMAE = [np.round(obj,decimals=2) for obj in np.ravel(np.asarray(allmodlMAE),'F')]\n",
    "storestuffMAE = np.vstack([np.asarray(obj) for obj in [alllistMAE,laglistMAE,randomlistMAE,xailistMAE]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c23b6f7a-eb27-4d80-bc7d-000fbdb0733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg_funcs.save_to_pickle('./store/rmse_mae/storestuff_noncausalmlr_mae.pkl',storestuffMAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
