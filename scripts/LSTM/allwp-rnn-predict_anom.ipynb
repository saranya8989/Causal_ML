{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6899627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/models.py:29: UserWarning: [Errno 2] No such file or directory: '/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/plotting.py:26: UserWarning: [Errno 2] No such file or directory: '/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/independence_tests/gpdc.py:27: UserWarning: [Errno 2] No such file or directory: '/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/independence_tests/../../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/independence_tests/gpdc_torch.py:33: UserWarning: [Errno 2] No such file or directory: '/opt/miniconda/envs/PySing/lib/python3.8/site-packages/tigramite/independence_tests/../../versions.py'\n",
      "  warnings.warn(str(e))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline     \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob,os\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import optuna\n",
    "from torchsummary import summary\n",
    "\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "from tigramite.models import Models, Prediction\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, GPDC, CMIknn, CMIsymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef97686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_links(tau_min, tau_max, parents, children):\n",
    "    \"\"\"\n",
    "    This function selects the causal links that will be tested by\n",
    "    PCMCI. The links are selected such that per each variable in\n",
    "    `children` all `parents` are stablished as causes, and no other\n",
    "    causal relationships exist.\n",
    "    \n",
    "    Assumes `parents` and `children` are disjoint sets, and that all\n",
    "    variables are included in the union of both sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tau_min : int\n",
    "        Minimum time lag to test. Note that zero-lags are undirected.\n",
    "    tau_max : int\n",
    "        Maximum time lag. Must be larger or equal to tau_min.\n",
    "    parents : set of int\n",
    "        List of variables that will be assigned as a parent link.\n",
    "        Assumed to be disjoint with children\n",
    "    children : set of int\n",
    "        List of variables that will be assigned a link from a parent.\n",
    "        Assumed to be disjoint with parents\n",
    "    Returns\n",
    "    -------\n",
    "    selected_links: dict\n",
    "        Dictionary of selected links for Tigramite\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    parents = set(parents)\n",
    "    children = set(children)\n",
    "\n",
    "    selected_links = dict()\n",
    "    # Set the default as all combinations of the selected variables\n",
    "    for var in [*children, *parents]:\n",
    "        if var in children:\n",
    "            # Children can be caused only by parents and by themselves\n",
    "            selected_links[var] = [\n",
    "                (parent, -lag)\n",
    "                for parent in parents\n",
    "                for lag in range(tau_min, tau_max + 1)\n",
    "            ]\n",
    "        else:\n",
    "            selected_links[var] = []\n",
    "\n",
    "    return selected_links\n",
    "\n",
    "def _process_dataset(path=None):\n",
    "    df1 = pd.read_csv(path,sep=',')\n",
    "    df1.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "    df1=df1.drop('a', axis=1)\n",
    "    df1=df1.drop('conv_rrate', axis=1)\n",
    "    df1=df1.drop('ls_rrate', axis=1)\n",
    "    df1=df1.drop('mn_conv_prate', axis=1)\n",
    "    df1=df1.drop('mn_ls_prate', axis=1)\n",
    "    df1=df1.drop('mn_tot_prate', axis=1)\n",
    "    df1=df1.drop('outconv_rrate', axis=1)\n",
    "    df1=df1.drop('outls_rrate', axis=1)\n",
    "    df1=df1.drop('outmn_conv_prate', axis=1)\n",
    "    df1=df1.drop('outmn_ls_prate', axis=1)\n",
    "    df1=df1.drop('outmn_tot_prate', axis=1)\n",
    "    df1=df1.drop('conv_ppt', axis=1)\n",
    "    df1=df1.drop('outconv_ppt', axis=1)\n",
    "    \n",
    "    TCname = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    #print(TCname)\n",
    "    for item in glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/targets/*tot_ppt_int*'):\n",
    "        if str(TCname) in item:\n",
    "            d1=pd.read_csv(item)\n",
    "            d1.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "            d1=d1.drop('a', axis=1)\n",
    "            dt1=pd.concat([d1,df1],axis=1, join='inner')\n",
    "        else:\n",
    "            continue\n",
    "    return dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255b15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=\"../../timeseries_csv/ts_wp/\"\n",
    "p2=\"../../../targets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8fec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1=_process_dataset(glob.glob(p1+'*2020vongfong*')[0])\n",
    "ds2=_process_dataset(glob.glob(p1+'*2020chanhom*')[0])\n",
    "ds3=_process_dataset(glob.glob(p1+'*2020saudel*')[0])\n",
    "ds4=_process_dataset(glob.glob(p1+'*2020molave*')[0])\n",
    "ds5=_process_dataset(glob.glob(p1+'*2020goni*')[0])\n",
    "ds6=_process_dataset(glob.glob(p1+'*2020atsani*')[0])\n",
    "ds7=_process_dataset(glob.glob(p1+'*2020vamco*')[0])\n",
    "ds8=_process_dataset(glob.glob(p1+'*2019neoguri*')[0])\n",
    "ds9=_process_dataset(glob.glob(p1+'*2019bualoi*')[0])\n",
    "ds10=_process_dataset(glob.glob(p1+'*2019halong*')[0])\n",
    "ds11=_process_dataset(glob.glob(p1+'*2019nakri*')[0])\n",
    "ds12=_process_dataset(glob.glob(p1+'*2019fengshen*')[0])\n",
    "ds13=_process_dataset(glob.glob(p1+'*2019kalmaegi*')[0])\n",
    "ds14=_process_dataset(glob.glob(p1+'*2019fungwong*')[0])\n",
    "ds15=_process_dataset(glob.glob(p1+'*2019kammuri*')[0])\n",
    "ds16=_process_dataset(glob.glob(p1+'*2018jelawat*')[0])\n",
    "ds17=_process_dataset(glob.glob(p1+'*2018maliksi*')[0])\n",
    "ds18=_process_dataset(glob.glob(p1+'*2018kongrey*')[0])\n",
    "ds19=_process_dataset(glob.glob(p1+'*2018yutu*')[0])\n",
    "ds20=_process_dataset(glob.glob(p1+'*2017muifa*')[0])\n",
    "ds21=_process_dataset(glob.glob(p1+'*2017lan*')[0])\n",
    "ds22=_process_dataset(glob.glob(p1+'*2017haikul*')[0])\n",
    "ds23=_process_dataset(glob.glob(p1+'*2016megi*')[0])\n",
    "ds24=_process_dataset(glob.glob(p1+'*2016sarika*')[0])\n",
    "ds25=_process_dataset(glob.glob(p1+'*2016haima*')[0])\n",
    "ds26=_process_dataset(glob.glob(p1+'*2015maysak*')[0])\n",
    "ds27=_process_dataset(glob.glob(p1+'*2015koppu*')[0])\n",
    "ds28=_process_dataset(glob.glob(p1+'*2015infa*')[0])\n",
    "ds29=_process_dataset(glob.glob(p1+'*2014tapah*')[0])\n",
    "ds30=_process_dataset(glob.glob(p1+'*2014nuri*')[0])\n",
    "ds31=_process_dataset(glob.glob(p1+'*2014hagupit*')[0])\n",
    "ds32=_process_dataset(glob.glob(p1+'*2013yagi*')[0])\n",
    "ds33=_process_dataset(glob.glob(p1+'*2013fitow*')[0])\n",
    "ds34=_process_dataset(glob.glob(p1+'*2013danas*')[0])\n",
    "ds35=_process_dataset(glob.glob(p1+'*2013francisco*')[0])\n",
    "ds36=_process_dataset(glob.glob(p1+'*2013krosa*')[0])\n",
    "ds37=_process_dataset(glob.glob(p1+'*2013haiyan*')[0])\n",
    "ds38=_process_dataset(glob.glob(p1+'*2012guchol*')[0])\n",
    "ds39=_process_dataset(glob.glob(p1+'*2012gaemi*')[0])\n",
    "ds40=_process_dataset(glob.glob(p1+'*2012maria*')[0])\n",
    "ds41=_process_dataset(glob.glob(p1+'*2012sontinh*')[0])\n",
    "ds42=_process_dataset(glob.glob(p1+'*2012bopha*')[0])\n",
    "ds43=_process_dataset(glob.glob(p1+'*2011songda*')[0])\n",
    "ds44=_process_dataset(glob.glob(p1+'*2011haima*')[0])\n",
    "ds45=_process_dataset(glob.glob(p1+'*2011nalgae*')[0])\n",
    "ds46=_process_dataset(glob.glob(p1+'*2011washi*')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a52364-0d00-4c56-a284-a8f1cc15ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds47=_process_dataset(glob.glob(p1+'*2010OMAIS*')[0])\n",
    "ds48=_process_dataset(glob.glob(p1+'*2010CONSON*')[0])\n",
    "ds49=_process_dataset(glob.glob(p1+'*2010CHANTHU*')[0])\n",
    "ds50=_process_dataset(glob.glob(p1+'*2010DIANMU*')[0])\n",
    "ds51=_process_dataset(glob.glob(p1+'*2010LIONROCK*')[0])\n",
    "ds52=_process_dataset(glob.glob(p1+'*2010MALOU*')[0])\n",
    "ds53=_process_dataset(glob.glob(p1+'*2010FANAPI*')[0])\n",
    "ds54=_process_dataset(glob.glob(p1+'*2010MALAKAS*')[0])\n",
    "ds55=_process_dataset(glob.glob(p1+'*2010MEGI*')[0])\n",
    "ds56=_process_dataset(glob.glob(p1+'*2010CHABA*')[0])\n",
    "ds57=_process_dataset(glob.glob(p1+'*2010OMEKA*')[0])\n",
    "ds58=_process_dataset(glob.glob(p1+'*2009KUJIRA*')[0])\n",
    "ds59=_process_dataset(glob.glob(p1+'*2009CHAN-HOM*')[0])\n",
    "ds60=_process_dataset(glob.glob(p1+'*2009LINFA*')[0])\n",
    "ds61=_process_dataset(glob.glob(p1+'*2009MORAKOT*')[0])\n",
    "ds62=_process_dataset(glob.glob(p1+'*2009ETAU*')[0])\n",
    "ds63=_process_dataset(glob.glob(p1+'*2009VAMCO*')[0])\n",
    "ds64=_process_dataset(glob.glob(p1+'*2009KROVANH*')[0])\n",
    "ds65=_process_dataset(glob.glob(p1+'*2009DUJUAN*')[0])\n",
    "ds66=_process_dataset(glob.glob(p1+'*2009CHOI-WAN*')[0])\n",
    "ds67=_process_dataset(glob.glob(p1+'*2009PARMA*')[0])\n",
    "ds68=_process_dataset(glob.glob(p1+'*2009MELOR*')[0])\n",
    "ds69=_process_dataset(glob.glob(p1+'*2009LUPIT*')[0])\n",
    "ds70=_process_dataset(glob.glob(p1+'*2009MIRINAE*')[0])\n",
    "ds71=_process_dataset(glob.glob(p1+'*2009NIDA*')[0])\n",
    "ds72=_process_dataset(glob.glob(p1+'*2008NEOGURI*')[0])\n",
    "ds73=_process_dataset(glob.glob(p1+'*2008RAMMASUN*')[0])\n",
    "ds74=_process_dataset(glob.glob(p1+'*2008NAKRI*')[0])\n",
    "ds75=_process_dataset(glob.glob(p1+'*2008FENGSHEN*')[0])\n",
    "ds76=_process_dataset(glob.glob(p1+'*2008KALMAEGI*')[0])\n",
    "ds77=_process_dataset(glob.glob(p1+'*2008FUNG-WONG*')[0])\n",
    "ds78=_process_dataset(glob.glob(p1+'*2008VONGFONG*')[0])\n",
    "ds79=_process_dataset(glob.glob(p1+'*2008NURI*')[0])\n",
    "ds80=_process_dataset(glob.glob(p1+'*2008SINLAKU*')[0])\n",
    "ds81=_process_dataset(glob.glob(p1+'*2008HAGUPIT*')[0])\n",
    "ds82=_process_dataset(glob.glob(p1+'*2008JANGMI*')[0])\n",
    "ds83=_process_dataset(glob.glob(p1+'*2008HIGOS*')[0])\n",
    "ds84=_process_dataset(glob.glob(p1+'*2008MAYSAK*')[0])\n",
    "ds85=_process_dataset(glob.glob(p1+'*2008DOLPHIN*')[0])\n",
    "ds86=_process_dataset(glob.glob(p1+'*2007KONG-REY*')[0])\n",
    "ds87=_process_dataset(glob.glob(p1+'*2007MAN-YI*')[0])\n",
    "ds88=_process_dataset(glob.glob(p1+'*2007USAGI*')[0])\n",
    "ds89=_process_dataset(glob.glob(p1+'*2007PABUK*')[0])\n",
    "ds90=_process_dataset(glob.glob(p1+'*2007SEPAT*')[0])\n",
    "ds91=_process_dataset(glob.glob(p1+'*2007FITOW*')[0])\n",
    "ds92=_process_dataset(glob.glob(p1+'*2007DANAS*')[0])\n",
    "ds93=_process_dataset(glob.glob(p1+'*2007NARI*')[0])\n",
    "ds94=_process_dataset(glob.glob(p1+'*2007WIPHA*')[0])\n",
    "ds95=_process_dataset(glob.glob(p1+'*2007LEKIMA*')[0])\n",
    "ds96=_process_dataset(glob.glob(p1+'*2007KROSA*')[0])\n",
    "ds97=_process_dataset(glob.glob(p1+'*2007LINGLING*')[0])\n",
    "ds98=_process_dataset(glob.glob(p1+'*2007PEIPAH*')[0])\n",
    "ds99=_process_dataset(glob.glob(p1+'*2007HAGIBIS*')[0])\n",
    "ds100=_process_dataset(glob.glob(p1+'*2007MITAG*')[0])\n",
    "ds101=_process_dataset(glob.glob(p1+'*2006CHANCHU*')[0])\n",
    "ds102=_process_dataset(glob.glob(p1+'*2006EWINIAR*')[0])\n",
    "ds103=_process_dataset(glob.glob(p1+'*2006BILIS*')[0])\n",
    "ds104=_process_dataset(glob.glob(p1+'*2006KAEMI*')[0])\n",
    "ds105=_process_dataset(glob.glob(p1+'*2006PRAPIROON*')[0])\n",
    "ds106=_process_dataset(glob.glob(p1+'*2006SAOMAI*')[0])\n",
    "ds107=_process_dataset(glob.glob(p1+'*2006WUKONG*')[0])\n",
    "ds108=_process_dataset(glob.glob(p1+'*2006IOKE*')[0])\n",
    "ds109=_process_dataset(glob.glob(p1+'*2006SHANSHAN*')[0])\n",
    "ds110=_process_dataset(glob.glob(p1+'*2006MUKDA*')[0])\n",
    "ds111=_process_dataset(glob.glob(p1+'*2006XANGSANE*')[0])\n",
    "ds112=_process_dataset(glob.glob(p1+'*2006BEBINCA*')[0])\n",
    "ds113=_process_dataset(glob.glob(p1+'*2006SOULIK*')[0])\n",
    "ds114=_process_dataset(glob.glob(p1+'*2006CIMARON*')[0])\n",
    "ds115=_process_dataset(glob.glob(p1+'*2006CHEBI*')[0])\n",
    "ds116=_process_dataset(glob.glob(p1+'*2006DURIAN*')[0])\n",
    "ds117=_process_dataset(glob.glob(p1+'*2006UTOR*')[0])\n",
    "ds118=_process_dataset(glob.glob(p1+'*2005KULAP*')[0])\n",
    "ds119=_process_dataset(glob.glob(p1+'*2005ROKE*')[0])\n",
    "ds120=_process_dataset(glob.glob(p1+'*2005SONCA*')[0])\n",
    "ds121=_process_dataset(glob.glob(p1+'*2005HAITANG*')[0])\n",
    "ds122=_process_dataset(glob.glob(p1+'*2005NALGAE*')[0])\n",
    "ds123=_process_dataset(glob.glob(p1+'*2005BANYAN*')[0])\n",
    "ds124=_process_dataset(glob.glob(p1+'*2005MATSA*')[0])\n",
    "ds125=_process_dataset(glob.glob(p1+'*2005GUCHOL*')[0])\n",
    "ds126=_process_dataset(glob.glob(p1+'*2005MAWAR*')[0])\n",
    "ds127=_process_dataset(glob.glob(p1+'*2005TALIM*')[0])\n",
    "ds128=_process_dataset(glob.glob(p1+'*2005NABI*')[0])\n",
    "ds129=_process_dataset(glob.glob(p1+'*2005KHANUN*')[0])\n",
    "ds130=_process_dataset(glob.glob(p1+'*2005DAMREY*')[0])\n",
    "ds131=_process_dataset(glob.glob(p1+'*2005SAOLA*')[0])\n",
    "ds132=_process_dataset(glob.glob(p1+'*2005LONGWANG*')[0])\n",
    "ds133=_process_dataset(glob.glob(p1+'*2005KIROGI*')[0])\n",
    "ds134=_process_dataset(glob.glob(p1+'*2005TEMBIN*')[0])\n",
    "ds135=_process_dataset(glob.glob(p1+'*2005BOLAVEN*')[0])\n",
    "ds136=_process_dataset(glob.glob(p1+'*2004SUDAL*')[0])\n",
    "ds137=_process_dataset(glob.glob(p1+'*2004NIDA*')[0])\n",
    "ds138=_process_dataset(glob.glob(p1+'*2004CHANTHU*')[0])\n",
    "ds139=_process_dataset(glob.glob(p1+'*2004DIANMU*')[0])\n",
    "ds140=_process_dataset(glob.glob(p1+'*2004MINDULLE*')[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a256ce0-4080-419b-b5d4-e944d89f7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds141=_process_dataset(glob.glob(p1+'*2004TINGTING*')[0])\n",
    "ds142=_process_dataset(glob.glob(p1+'*2004NAMTHEUN*')[0])\n",
    "ds143=_process_dataset(glob.glob(p1+'*2004MERANTI*')[0])\n",
    "ds144=_process_dataset(glob.glob(p1+'*2004RANANIM*')[0])\n",
    "ds145=_process_dataset(glob.glob(p1+'*2004MEGI*')[0])\n",
    "ds146=_process_dataset(glob.glob(p1+'*2004CHABA*')[0])\n",
    "ds147=_process_dataset(glob.glob(p1+'*2004AERE*')[0])\n",
    "ds148=_process_dataset(glob.glob(p1+'*2004SONGDA*')[0])\n",
    "ds149=_process_dataset(glob.glob(p1+'*2004MEARI*')[0])\n",
    "ds150=_process_dataset(glob.glob(p1+'*2004MA-ON*')[0])\n",
    "ds151=_process_dataset(glob.glob(p1+'*2004TOKAGE*')[0])\n",
    "ds152=_process_dataset(glob.glob(p1+'*2004NOCK-TEN*')[0])\n",
    "ds153=_process_dataset(glob.glob(p1+'*2004MUIFA*')[0])\n",
    "ds154=_process_dataset(glob.glob(p1+'*2004NANMADOL*')[0])\n",
    "ds155=_process_dataset(glob.glob(p1+'*2004NORU*')[0])\n",
    "ds156=_process_dataset(glob.glob(p1+'*2003SOUDELOR*')[0])\n",
    "ds157=_process_dataset(glob.glob(p1+'*2003KUJIRA*')[0])\n",
    "ds158=_process_dataset(glob.glob(p1+'*2003CHAN-HOM*')[0])\n",
    "ds159=_process_dataset(glob.glob(p1+'*2003IMBUDO*')[0])\n",
    "ds160=_process_dataset(glob.glob(p1+'*2003NANGKA*')[0])\n",
    "ds161=_process_dataset(glob.glob(p1+'*2003KONI*')[0])\n",
    "ds162=_process_dataset(glob.glob(p1+'*2003ETAU*')[0])\n",
    "ds163=_process_dataset(glob.glob(p1+'*2003KROVANH*')[0])\n",
    "ds164=_process_dataset(glob.glob(p1+'*2003DUJUAN*')[0])\n",
    "ds165=_process_dataset(glob.glob(p1+'*2003MAEMI*')[0])\n",
    "ds166=_process_dataset(glob.glob(p1+'*2003CHOI-WAN*')[0])\n",
    "ds167=_process_dataset(glob.glob(p1+'*2003KOPPU*')[0])\n",
    "ds168=_process_dataset(glob.glob(p1+'*2003KETSANA*')[0])\n",
    "ds169=_process_dataset(glob.glob(p1+'*2003MELOR*')[0])\n",
    "ds170=_process_dataset(glob.glob(p1+'*2003NEPARTAK*')[0])\n",
    "ds171=_process_dataset(glob.glob(p1+'*2003LUPIT*')[0])\n",
    "ds172=_process_dataset(glob.glob(p1+'*2002MITAG*')[0])\n",
    "ds173=_process_dataset(glob.glob(p1+'*2002HAGIBIS*')[0])\n",
    "ds174=_process_dataset(glob.glob(p1+'*2002CHATAAN*')[0])\n",
    "ds175=_process_dataset(glob.glob(p1+'*2002RAMMASUN*')[0])\n",
    "ds176=_process_dataset(glob.glob(p1+'*2002HALONG*')[0])\n",
    "ds177=_process_dataset(glob.glob(p1+'*2002FENGSHEN*')[0])\n",
    "ds178=_process_dataset(glob.glob(p1+'*2002FUNG-WONG*')[0])\n",
    "ds179=_process_dataset(glob.glob(p1+'*2002PHANFONE*')[0])\n",
    "ds180=_process_dataset(glob.glob(p1+'*2002RUSA*')[0])\n",
    "ds181=_process_dataset(glob.glob(p1+'*2002ELE*')[0])\n",
    "ds182=_process_dataset(glob.glob(p1+'*2002SINLAKU*')[0])\n",
    "ds183=_process_dataset(glob.glob(p1+'*2002HAGUPIT*')[0])\n",
    "ds184=_process_dataset(glob.glob(p1+'*2002MEKKHALA*')[0])\n",
    "ds185=_process_dataset(glob.glob(p1+'*2002HIGOS*')[0])\n",
    "ds186=_process_dataset(glob.glob(p1+'*2002BAVI*')[0])\n",
    "ds187=_process_dataset(glob.glob(p1+'*2002HUKO*')[0])\n",
    "ds188=_process_dataset(glob.glob(p1+'*2002HAISHEN*')[0])\n",
    "ds189=_process_dataset(glob.glob(p1+'*2002PONGSONA*')[0])\n",
    "ds190=_process_dataset(glob.glob(p1+'*2001CIMARON*')[0])\n",
    "ds191=_process_dataset(glob.glob(p1+'*2001CHEBI*')[0])\n",
    "ds192=_process_dataset(glob.glob(p1+'*2001UTOR*')[0])\n",
    "ds193=_process_dataset(glob.glob(p1+'*2001KONG-REY*')[0])\n",
    "ds194=_process_dataset(glob.glob(p1+'*2001FRANCISCO*')[0])\n",
    "ds195=_process_dataset(glob.glob(p1+'*2001MAN-YI*')[0])\n",
    "ds196=_process_dataset(glob.glob(p1+'*2001PABUK*')[0])\n",
    "ds197=_process_dataset(glob.glob(p1+'*2001WUTIP*')[0])\n",
    "ds198=_process_dataset(glob.glob(p1+'*2001FITOW*')[0])\n",
    "ds199=_process_dataset(glob.glob(p1+'*2001NARI*')[0])\n",
    "ds200=_process_dataset(glob.glob(p1+'*2001VIPA*')[0])\n",
    "ds201=_process_dataset(glob.glob(p1+'*2001FRANCISCO*')[0])\n",
    "ds202=_process_dataset(glob.glob(p1+'*2001LEKIMA*')[0])\n",
    "ds203=_process_dataset(glob.glob(p1+'*2001KROSA*')[0])\n",
    "ds204=_process_dataset(glob.glob(p1+'*2001HAIYAN*')[0])\n",
    "ds205=_process_dataset(glob.glob(p1+'*2001PODUL*')[0])\n",
    "ds206=_process_dataset(glob.glob(p1+'*2001LINGLING*')[0])\n",
    "ds207=_process_dataset(glob.glob(p1+'*2001FAXAI*')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e7d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcwp1=ds1.values\n",
    "tcwp2=ds2.values\n",
    "tcwp3=ds3.values\n",
    "tcwp4=ds4.values\n",
    "tcwp5=ds5.values\n",
    "tcwp6=ds6.values\n",
    "tcwp7=ds7.values\n",
    "tcwp8=ds8.values\n",
    "tcwp9=ds9.values\n",
    "tcwp10=ds10.values\n",
    "tcwp11=ds11.values\n",
    "tcwp12=ds12.values\n",
    "tcwp13=ds13.values\n",
    "tcwp14=ds14.values\n",
    "tcwp15=ds15.values\n",
    "tcwp16=ds16.values\n",
    "tcwp17=ds17.values\n",
    "tcwp18=ds18.values\n",
    "tcwp19=ds19.values\n",
    "tcwp20=ds20.values\n",
    "tcwp21=ds21.values\n",
    "tcwp22=ds22.values\n",
    "tcwp23=ds23.values\n",
    "tcwp24=ds24.values\n",
    "tcwp25=ds25.values\n",
    "tcwp26=ds26.values\n",
    "tcwp27=ds27.values\n",
    "tcwp28=ds28.values\n",
    "tcwp29=ds29.values\n",
    "tcwp30=ds30.values\n",
    "tcwp31=ds31.values\n",
    "tcwp32=ds32.values\n",
    "tcwp33=ds33.values\n",
    "tcwp34=ds34.values\n",
    "tcwp35=ds35.values\n",
    "tcwp36=ds36.values\n",
    "tcwp37=ds37.values\n",
    "tcwp38=ds38.values\n",
    "tcwp39=ds39.values\n",
    "tcwp40=ds40.values\n",
    "tcwp41=ds41.values\n",
    "tcwp42=ds42.values\n",
    "tcwp43=ds43.values\n",
    "tcwp44=ds44.values\n",
    "tcwp45=ds45.values\n",
    "tcwp46=ds46.values\n",
    "tcwp47=ds47.values\n",
    "tcwp48=ds48.values\n",
    "tcwp49=ds49.values\n",
    "tcwp50=ds50.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80c9b21-854d-44fd-bcc0-4f9598defa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcwp51=ds51.values\n",
    "tcwp52=ds52.values\n",
    "tcwp53=ds53.values\n",
    "tcwp54=ds54.values\n",
    "tcwp55=ds55.values\n",
    "tcwp56=ds56.values\n",
    "tcwp57=ds57.values\n",
    "tcwp58=ds58.values\n",
    "tcwp59=ds59.values\n",
    "tcwp60=ds60.values\n",
    "tcwp61=ds61.values\n",
    "tcwp62=ds62.values\n",
    "tcwp63=ds63.values\n",
    "tcwp64=ds64.values\n",
    "tcwp65=ds65.values\n",
    "tcwp66=ds66.values\n",
    "tcwp67=ds67.values\n",
    "tcwp68=ds68.values\n",
    "tcwp69=ds69.values\n",
    "tcwp70=ds70.values\n",
    "tcwp71=ds71.values\n",
    "tcwp72=ds72.values\n",
    "tcwp73=ds73.values\n",
    "tcwp74=ds74.values\n",
    "tcwp75=ds75.values\n",
    "tcwp76=ds76.values\n",
    "tcwp77=ds77.values\n",
    "tcwp78=ds78.values\n",
    "tcwp79=ds79.values\n",
    "tcwp80=ds80.values\n",
    "tcwp81=ds81.values\n",
    "tcwp82=ds82.values\n",
    "tcwp83=ds83.values\n",
    "tcwp84=ds84.values\n",
    "tcwp85=ds85.values\n",
    "tcwp86=ds86.values\n",
    "tcwp87=ds87.values\n",
    "tcwp88=ds88.values\n",
    "tcwp89=ds89.values\n",
    "tcwp90=ds90.values\n",
    "tcwp91=ds91.values\n",
    "tcwp92=ds92.values\n",
    "tcwp93=ds93.values\n",
    "tcwp94=ds94.values\n",
    "tcwp95=ds95.values\n",
    "tcwp96=ds96.values\n",
    "tcwp97=ds97.values\n",
    "tcwp98=ds98.values\n",
    "tcwp99=ds99.values\n",
    "tcwp100=ds100.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f604e12-01ec-47ef-9b63-6ba8702f7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcwp101=ds101.values\n",
    "tcwp102=ds102.values\n",
    "tcwp103=ds103.values\n",
    "tcwp104=ds104.values\n",
    "tcwp105=ds105.values\n",
    "tcwp106=ds106.values\n",
    "tcwp107=ds107.values\n",
    "tcwp108=ds108.values\n",
    "tcwp109=ds109.values\n",
    "tcwp110=ds110.values\n",
    "tcwp111=ds111.values\n",
    "tcwp112=ds112.values\n",
    "tcwp113=ds113.values\n",
    "tcwp114=ds114.values\n",
    "tcwp115=ds115.values\n",
    "tcwp116=ds116.values\n",
    "tcwp117=ds117.values\n",
    "tcwp118=ds118.values\n",
    "tcwp119=ds119.values\n",
    "tcwp120=ds120.values\n",
    "tcwp121=ds121.values\n",
    "tcwp122=ds122.values\n",
    "tcwp123=ds123.values\n",
    "tcwp124=ds124.values\n",
    "tcwp125=ds125.values\n",
    "tcwp126=ds126.values\n",
    "tcwp127=ds127.values\n",
    "tcwp128=ds128.values\n",
    "tcwp129=ds129.values\n",
    "tcwp130=ds130.values\n",
    "tcwp131=ds131.values\n",
    "tcwp132=ds132.values\n",
    "tcwp133=ds133.values\n",
    "tcwp134=ds134.values\n",
    "tcwp135=ds135.values\n",
    "tcwp136=ds136.values\n",
    "tcwp137=ds137.values\n",
    "tcwp138=ds138.values\n",
    "tcwp139=ds139.values\n",
    "tcwp140=ds140.values\n",
    "tcwp141=ds141.values\n",
    "tcwp142=ds142.values\n",
    "tcwp143=ds143.values\n",
    "tcwp144=ds144.values\n",
    "tcwp145=ds145.values\n",
    "tcwp146=ds146.values\n",
    "tcwp147=ds147.values\n",
    "tcwp148=ds148.values\n",
    "tcwp149=ds149.values\n",
    "tcwp150=ds150.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9ef44c-624b-405d-837a-7bf1a4fa3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcwp151=ds151.values\n",
    "tcwp152=ds152.values\n",
    "tcwp153=ds153.values\n",
    "tcwp154=ds154.values\n",
    "tcwp155=ds155.values\n",
    "tcwp156=ds156.values\n",
    "tcwp157=ds157.values\n",
    "tcwp158=ds158.values\n",
    "tcwp159=ds159.values\n",
    "tcwp160=ds160.values\n",
    "tcwp161=ds161.values\n",
    "tcwp162=ds162.values\n",
    "tcwp163=ds163.values\n",
    "tcwp164=ds164.values\n",
    "tcwp165=ds165.values\n",
    "tcwp166=ds166.values\n",
    "tcwp167=ds167.values\n",
    "tcwp168=ds168.values\n",
    "tcwp169=ds169.values\n",
    "tcwp170=ds170.values\n",
    "tcwp171=ds171.values\n",
    "tcwp172=ds172.values\n",
    "tcwp173=ds173.values\n",
    "tcwp174=ds174.values\n",
    "tcwp175=ds175.values\n",
    "tcwp176=ds176.values\n",
    "tcwp177=ds177.values\n",
    "tcwp178=ds178.values\n",
    "tcwp179=ds179.values\n",
    "tcwp180=ds180.values\n",
    "tcwp181=ds181.values\n",
    "tcwp182=ds182.values\n",
    "tcwp183=ds183.values\n",
    "tcwp184=ds184.values\n",
    "tcwp185=ds185.values\n",
    "tcwp186=ds186.values\n",
    "tcwp187=ds187.values\n",
    "tcwp188=ds188.values\n",
    "tcwp189=ds189.values\n",
    "tcwp190=ds190.values\n",
    "tcwp191=ds191.values\n",
    "tcwp192=ds192.values\n",
    "tcwp193=ds193.values\n",
    "tcwp194=ds194.values\n",
    "tcwp195=ds195.values\n",
    "tcwp196=ds196.values\n",
    "tcwp197=ds197.values\n",
    "tcwp198=ds198.values\n",
    "tcwp199=ds199.values\n",
    "tcwp200=ds200.values\n",
    "tcwp201=ds201.values\n",
    "tcwp202=ds202.values\n",
    "tcwp203=ds203.values\n",
    "tcwp204=ds204.values\n",
    "tcwp205=ds205.values\n",
    "tcwp206=ds206.values\n",
    "tcwp207=ds207.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35a809",
   "metadata": {},
   "source": [
    "West Pacific Storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e304eda6-e120-4235-9348-059efe2d1d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddwp={'cyclone2':tcwp2,'cyclone3':tcwp3,'cyclone4':tcwp4,'cyclone5':tcwp5,'cyclone6':tcwp6,\n",
    "      'cyclone7':tcwp7, 'cyclone8':tcwp8,'cyclone9':tcwp9,'cyclone10':tcwp10,'cyclone11':tcwp11,'cyclone12':tcwp12,\n",
    "      'cyclone13':tcwp13,'cyclone14':tcwp14,'cyclone15':tcwp15,'cyclone16':tcwp16,'cyclone17':tcwp17,\n",
    "      'cyclone18':tcwp18,'cyclone19':tcwp19,'cyclone21':tcwp21,'cyclone22':tcwp22,'cyclone23':tcwp23,'cyclone24':tcwp24,\n",
    "      'cyclone25':tcwp25,'cyclone26':tcwp26,'cyclone27':tcwp27,'cyclone28':tcwp28,'cyclone29':tcwp29,'cyclone30':tcwp30,\n",
    "      'cyclone31':tcwp31,'cyclone32':tcwp32,'cyclone33':tcwp33,'cyclone34':tcwp34,'cyclone35':tcwp35,'cyclone36':tcwp36,\n",
    "      'cyclone37':tcwp37,'cyclone38':tcwp38,'cyclone39':tcwp39,'cyclone40':tcwp40,'cyclone41':tcwp41,'cyclone42':tcwp42,\n",
    "      'cyclone43':tcwp43,'cyclone44':tcwp44,'cyclone45':tcwp45,'cyclone46':tcwp46,'cyclone47':tcwp47,'cyclone48':tcwp48,\n",
    "      'cyclone49':tcwp49,'cyclone50':tcwp50,'cyclone51':tcwp51,\n",
    "      'cyclone52':tcwp52, 'cyclone53':tcwp53,'cyclone54':tcwp54,'cyclone55':tcwp55,'cyclone56':tcwp56,'cyclone57':tcwp57,\n",
    "      'cyclone58':tcwp58,'cyclone59':tcwp59,'cyclone60':tcwp60,'cyclone61':tcwp61,'cyclone62':tcwp62,'cyclone63':tcwp63,\n",
    "      'cyclone64':tcwp64,'cyclone65':tcwp65,'cyclone66':tcwp66,'cyclone67':tcwp67,'cyclone68':tcwp68,'cyclone69':tcwp69,\n",
    "      'cyclone70':tcwp70,'cyclone71':tcwp71,'cyclone72':tcwp72,'cyclone73':tcwp73,'cyclone74':tcwp74,'cyclone75':tcwp75,\n",
    "      'cyclone76':tcwp76,'cyclone77':tcwp77,'cyclone78':tcwp78,'cyclone79':tcwp79,'cyclone80':tcwp80,'cyclone81':tcwp81,\n",
    "      'cyclone82':tcwp82,'cyclone83':tcwp83,'cyclone84':tcwp84,'cyclone85':tcwp85,'cyclone86':tcwp86,'cyclone87':tcwp87,\n",
    "      'cyclone88':tcwp88,'cyclone89':tcwp89,'cyclone90':tcwp90,'cyclone91':tcwp91,'cyclone92':tcwp92,'cyclone93':tcwp93,\n",
    "      'cyclone94':tcwp94,'cyclone95':tcwp95,'cyclone96':tcwp96,'cyclone97':tcwp97,'cyclone98':tcwp98,\n",
    "      'cyclone99':tcwp99,'cyclone100':tcwp100,'cyclone101':tcwp101,'cyclone102':tcwp102,'cyclone103':tcwp103,'cyclone104':tcwp104,\n",
    "      'cyclone105':tcwp105,'cyclone106':tcwp106,'cyclone107':tcwp107,'cyclone108':tcwp108,'cyclone109':tcwp109,\n",
    "      'cyclone110':tcwp110,'cyclone111':tcwp111,'cyclone112':tcwp112,'cyclone113':tcwp113,'cyclone114':tcwp114,\n",
    "      'cyclone115':tcwp115,'cyclone116':tcwp116,'cyclone117':tcwp117,'cyclone118':tcwp118,'cyclone119':tcwp119,\n",
    "      'cyclone120':tcwp120,'cyclone121':tcwp121,'cyclone122':tcwp122,'cyclone123':tcwp123,'cyclone124':tcwp124,\n",
    "      'cyclone125':tcwp125,'cyclone126':tcwp126,'cyclone127':tcwp127,'cyclone128':tcwp128,'cyclone129':tcwp129,\n",
    "      'cyclone130':tcwp130,'cyclone131':tcwp131,'cyclone132':tcwp132,'cyclone133':tcwp133,'cyclone134':tcwp134,\n",
    "      'cyclone135':tcwp135,'cyclone136':tcwp136,'cyclone137':tcwp137,'cyclone138':tcwp138,'cyclone139':tcwp139,\n",
    "      'cyclone140':tcwp140,'cyclone141':tcwp141,'cyclone142':tcwp142,'cyclone143':tcwp143,'cyclone144':tcwp144,\n",
    "      'cyclone145':tcwp145,'cyclone146':tcwp146,'cyclone147':tcwp147,'cyclone148':tcwp148,'cyclone149':tcwp149,'cyclone150':tcwp150,\n",
    "      'cyclone151':tcwp151,'cyclone152':tcwp152,'cyclone153':tcwp153,'cyclone154':tcwp154,'cyclone155':tcwp155,\n",
    "      'cyclone156':tcwp156,'cyclone157':tcwp157,'cyclone158':tcwp158,'cyclone159':tcwp159,'cyclone160':tcwp160,\n",
    "      'cyclone161':tcwp161,'cyclone162':tcwp162,'cyclone163':tcwp163,'cyclone164':tcwp164,'cyclone165':tcwp165,\n",
    "      'cyclone166':tcwp166,'cyclone167':tcwp167,'cyclone168':tcwp168,'cyclone169':tcwp169,'cyclone170':tcwp170,\n",
    "      'cyclone171':tcwp171,'cyclone172':tcwp172,'cyclone173':tcwp173,'cyclone174':tcwp174,'cyclone175':tcwp175,\n",
    "      'cyclone176':tcwp176,'cyclone177':tcwp177,'cyclone178':tcwp178,'cyclone179':tcwp179,'cyclone180':tcwp180,\n",
    "      'cyclone181':tcwp181,'cyclone182':tcwp182,'cyclone183':tcwp183,'cyclone184':tcwp184,'cyclone185':tcwp185,\n",
    "      'cyclone186':tcwp186,'cyclone187':tcwp187,'cyclone188':tcwp188,'cyclone189':tcwp189,'cyclone190':tcwp190,\n",
    "      'cyclone191':tcwp191,'cyclone192':tcwp192,'cyclone193':tcwp193,'cyclone194':tcwp194,'cyclone195':tcwp195,\n",
    "      'cyclone196':tcwp196,'cylone197':tcwp197,'cyclone198':tcwp198,'cyclone199':tcwp199,'cyclone200':tcwp200,\n",
    "      'cyclone201':tcwp201,'cyclone202':tcwp202,'cyclone203':tcwp203,'cyclone204':tcwp204,'cyclone205':tcwp205,\n",
    "      'cyclone206':tcwp206,'cyclone207':tcwp207}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f30b85d-4f5e-4b52-ae74-17a58492c42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c8f4280da7436ebe6ef4c38d2c289d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "testcyclone_dict = {}\n",
    "newtestfilelist_f = natsorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/timeseries_csv/new_storms_wpac/*'))[-55-7:-7]\n",
    "for ind,obj in tqdm(enumerate(newtestfilelist_f)):\n",
    "    testcyclone_dict['testcyclone'+str(ind)] = _process_dataset(obj).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9a900-108d-43cb-9855-aa10a970f410",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3c72b-c189-477e-86da-bb903332cbf2",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99e53bd-7bf4-4962-b73f-49a8a859fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import rfreg_funcs\n",
    "class Pipeline_preproc:\n",
    "    \"\"\"\n",
    "    Tigramite and Linear Regression Pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self,data,pc_alpha,alpha_level,pc_type='run_pcstable' or 'pcmci',tau_min0=None,tau_max0=None,\n",
    "                 target='precip',var_name=None,seed=None,cond_ind_test=ParCorr()):\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.alpha_level = alpha_level\n",
    "        self.data = data\n",
    "        self.pc_type = pc_type\n",
    "        self.target = target\n",
    "        self.tau_min0 = tau_min0\n",
    "        self.tau_max0 = tau_max0\n",
    "        self.var_name = var_name\n",
    "        self.seed = seed\n",
    "        self.cond_ind_test = cond_ind_test\n",
    "        \n",
    "    #################################################################################\n",
    "    # Step 0: Split\n",
    "    #################################################################################\n",
    "    def splitdata(self,testindex=None):\n",
    "        datae = self.data.copy()\n",
    "        traindata = {}\n",
    "        testdata = {}\n",
    "        validdata = {}\n",
    "        validindex,newtestindex = testindex[:int(len(testindex)/2)],testindex[int(len(testindex)/2):]\n",
    "        for obj in datae.keys():\n",
    "            number = int(obj[7:])\n",
    "            if number in list(newtestindex):\n",
    "                testdata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "            elif number in list(validindex):\n",
    "                validdata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "            else:\n",
    "                traindata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "        return traindata,validdata,testdata\n",
    "    \n",
    "    def random_testindex(self,totalexp=None,testexp=None):\n",
    "        from numpy.random import default_rng\n",
    "        rng = default_rng(self.seed)\n",
    "        seed = rng.choice(totalexp, testexp, replace=False)\n",
    "        return seed\n",
    "    \n",
    "    def extract_lag_info(self,datar=None,varindex=None,lag=None):\n",
    "        temp = datar[:,varindex] # Full time series\n",
    "        store = []\n",
    "        for timeindex in range(len(temp)):\n",
    "            if timeindex < np.abs(lag):\n",
    "                store.append(np.nan)\n",
    "            elif timeindex > len(temp)-1-np.abs(lag):\n",
    "                store.append(np.nan)\n",
    "            else:\n",
    "                store.append(temp[timeindex-np.abs(lag)])\n",
    "        return store\n",
    "    \n",
    "    def normalize_timeseries(self,traindata=None,validdata=None,testdata=None,newtestdata=None):\n",
    "        def __normalize__(indata=None,trainmean=None,trainstd=None):\n",
    "            newindata = {}\n",
    "            for datakey in indata.keys():\n",
    "                putIn = []\n",
    "                for i in range(234):\n",
    "                    if i<=2:\n",
    "                        putIn.append(indata[datakey][:,i])\n",
    "                    elif i>=3:\n",
    "                        putIn.append((indata[datakey][:,i]-trainmean[i])/trainstd[i])\n",
    "                putInarray = np.asarray(putIn).transpose()\n",
    "                newindata[datakey] = putInarray\n",
    "            return newindata\n",
    "        \n",
    "        traindatac,validdatac,testdatac,newtestdatac = deepcopy(traindata),deepcopy(validdata),deepcopy(testdata),deepcopy(newtestdata)\n",
    "        trainmeans = [np.nanmean(rfreg_funcs.flatten([traindatac[key][:,int(index)] for key in traindatac.keys()])) for index in range(234)]\n",
    "        trainstds = [np.nanstd(rfreg_funcs.flatten([traindatac[key][:,int(index)] for key in traindatac.keys()])) for index in range(234)]\n",
    "        # Training set\n",
    "        norml_train = __normalize__(traindatac,trainmeans,trainstds)\n",
    "        norml_valid = __normalize__(validdatac,trainmeans,trainstds)\n",
    "        norml_test = __normalize__(testdatac,trainmeans,trainstds)\n",
    "        norml_newtest = __normalize__(newtestdatac,trainmeans,trainstds)\n",
    "        return {'train':norml_train,'valid':norml_valid,'test':norml_test,'newtest':norml_newtest}\n",
    "    \n",
    "    def chunk_the_data_X(self,cyclonedata=None,tau_min=8,tau_max=24,linknum=234):\n",
    "        storechunks = []\n",
    "        for i in range(linknum):\n",
    "            storechunks.append([cyclonedata[:,i][t0+tau_max-tau_max:t0+(tau_max-tau_min)] for t0 in range(len(cyclonedata[:,i][tau_max:]))])\n",
    "        return storechunks\n",
    "    \n",
    "    def create_X(self,data=None,tau_min=8,tau_max=24,linknum=234):\n",
    "        storedict = {}\n",
    "        for key in data.keys():\n",
    "            storedict[key] = self.chunk_the_data_X(data[key],tau_min,tau_max,linknum)\n",
    "        return storedict\n",
    "    \n",
    "    def create_y(self,data=None,tau_max=24):\n",
    "        storeYs = {}\n",
    "        for key in data.keys():\n",
    "            storeYs[key] = {'precip':data[key][:,0][tau_max:],'pmin':data[key][:,1][tau_max:],'v10':data[key][:,2][tau_max:]}\n",
    "        return storeYs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2160275-92f7-4b16-b5a0-b3248cf824b0",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "611d962d-ded0-410b-b39e-986f20a42725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import rfreg_funcs\n",
    "pc_alphaa,alpha_level,splitsize,seednum=0.01,1,55,12348\n",
    "pc_type = 'lagcorr'\n",
    "targetname='precip'\n",
    "var_names=ds1.columns.values.tolist()\n",
    "\n",
    "testindex = (Pipeline_preproc(ddwp,pc_alphaa,alpha_level,pc_type=pc_type,tau_min0=8,tau_max0=24,\\\n",
    "                      target=targetname,var_name=var_names,seed=seednum).random_testindex(205,splitsize))\n",
    "traindata,validdata,testdata = Pipeline_preproc(ddwp,pc_alphaa,alpha_level,pc_type=pc_type,tau_min0=8,tau_max0=24,\\\n",
    "                                        target=targetname,var_name=var_names,seed=seednum).splitdata(testindex)\n",
    "newtestdata = deepcopy(testcyclone_dict)\n",
    "\n",
    "# Normalize data with mean,std from training set\n",
    "norml_data = Pipeline_preproc(ddwp,pc_alphaa,alpha_level,pc_type=pc_type,tau_min0=8,tau_max0=24,\\\n",
    "                      target=targetname,var_name=var_names,seed=seednum).normalize_timeseries(traindata,validdata,testdata,newtestdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68e67a-2922-4da3-9774-942dffe3331d",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76ef52a6-71bd-4574-9492-02bbe7c075f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0-tau_max:t0-tau_min; t0\n",
    "tau_max=24\n",
    "tau_min=8\n",
    "storeXs = {}\n",
    "for objective in ['train','valid','test','newtest']:\n",
    "    storeXs[objective] = Pipeline_preproc(ddwp,pc_alphaa,alpha_level,pc_type=pc_type,tau_min0=8,tau_max0=24,\\\n",
    "                                          target=targetname,var_name=var_names,seed=seednum).create_X(norml_data[objective],tau_min,tau_max,234)\n",
    "storeYs = {}\n",
    "for objective in ['train','valid','test','newtest']:\n",
    "    storeYs[objective] = Pipeline_preproc(ddwp,pc_alphaa,alpha_level,pc_type=pc_type,tau_min0=8,tau_max0=24,\\\n",
    "                                          target=targetname,var_name=var_names,seed=seednum).create_y(norml_data[objective],tau_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0923db95-c217-4dea-86c0-34196b73c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainswapped = [np.swapaxes(np.swapaxes(np.asarray(storeXs['train'][key]),0,2),0,1) for key in storeXs['train'].keys()]\n",
    "Xvalidswapped = [np.swapaxes(np.swapaxes(np.asarray(storeXs['valid'][key]),0,2),0,1) for key in storeXs['valid'].keys()]\n",
    "Xtestswapped = [np.swapaxes(np.swapaxes(np.asarray(storeXs['test'][key]),0,2),0,1) for key in storeXs['test'].keys()]\n",
    "Xnewtestswapped = [np.swapaxes(np.swapaxes(np.asarray(storeXs['newtest'][key]),0,2),0,1) for key in storeXs['newtest'].keys()]\n",
    "###############################################################################################################################\n",
    "# X \n",
    "###############################################################################################################################\n",
    "X_train = np.concatenate([pbj for pbj in Xtrainswapped])\n",
    "X_valid = np.concatenate([pbj for pbj in Xvalidswapped])\n",
    "X_test = np.concatenate([pbj for pbj in Xtestswapped])\n",
    "X_newtest = np.concatenate([pbj for pbj in Xnewtestswapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b08c9da9-560f-43c8-8078-eb6bc565ac4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_mn=np.asarray(rfreg_funcs.flatten([storeYs['train'][key][targetname] for key in storeYs['train'].keys()])).mean()\n",
    "target_std=np.asarray(rfreg_funcs.flatten([storeYs['train'][key][targetname] for key in storeYs['train'].keys()])).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37683107-ee31-460f-be57-fdaa8fb6620b",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19189966-6752-4709-bb9f-ab271196ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf582ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                hidden_units,\n",
    "                dropout_rates):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.units = hidden_units\n",
    "        self.drates = dropout_rates\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(input_size = input_size,\n",
    "                                 hidden_size = self.units,\n",
    "                                 num_layers = 1,\n",
    "                                 bias = True,\n",
    "                                 batch_first = True)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p = self.drates)\n",
    "        \n",
    "        self.dense_layer = nn.Linear(in_features = self.units, \n",
    "                                     out_features = self.units)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features = self.units,\n",
    "                                      out_features=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output, (h_n, c_n) = self.lstm_layer(X)\n",
    "        \n",
    "        hidden_state = self.dropout_layer(h_n[0])\n",
    "        #print(f'1st Hidden {hidden_state.shape}')\n",
    "        hidden_state2 = torch.tanh(self.dense_layer(hidden_state))\n",
    "        #print(f'2nd Hidden {hidden_state2.shape}')\n",
    "        p_hat = torch.flatten(self.output_layer((hidden_state2)))\n",
    "        \n",
    "        return p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85fddef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_inputs = torch.FloatTensor(X_train).to(device)\n",
    "valid_inputs = torch.FloatTensor(X_valid).to(device)\n",
    "test_inputs = torch.FloatTensor(X_test).to(device)\n",
    "newtest_inputs = torch.FloatTensor(X_newtest).to(device)\n",
    "\n",
    "train_outputs = torch.FloatTensor((rfreg_funcs.flatten([storeYs['train'][key][str(targetname)] \n",
    "                                                       for key in storeYs['train'].keys()]) - target_mn)/target_std).to(device)\n",
    "valid_outputs = torch.FloatTensor((rfreg_funcs.flatten([storeYs['valid'][key][str(targetname)] \n",
    "                                                       for key in storeYs['valid'].keys()])- target_mn)/target_std).to(device)\n",
    "test_outputs = torch.FloatTensor((rfreg_funcs.flatten([storeYs['test'][key][str(targetname)] \n",
    "                                                       for key in storeYs['test'].keys()]) - target_mn)/target_std).to(device)\n",
    "newtest_outputs = torch.FloatTensor((rfreg_funcs.flatten([storeYs['newtest'][key][str(targetname)] \n",
    "                                                       for key in storeYs['newtest'].keys()])- target_mn)/target_std).to(device)\n",
    "\n",
    "train_data = TensorDataset(train_inputs,train_outputs)\n",
    "valid_data = TensorDataset(valid_inputs,valid_outputs)\n",
    "test_data  = TensorDataset(test_inputs,test_outputs)\n",
    "newtest_data = TensorDataset(newtest_inputs,newtest_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a925521",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset = train_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "valid_loader = DataLoader(dataset = valid_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "newtest_loader = DataLoader(dataset = newtest_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0acfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,dataloader,loss_func):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        #metric = 0\n",
    "        \n",
    "        global_sum = 0\n",
    "        label_size = 0\n",
    "        for features, labels in dataloader:\n",
    "            global_sum += labels.sum()\n",
    "            label_size += len(labels)\n",
    "            \n",
    "        global_mean = global_sum / label_size\n",
    "        for features, labels in dataloader:\n",
    "            pred = model(features)\n",
    "            batch_loss = loss_func(pred, labels)\n",
    "            #batch_metric = metric_func(pred, labels, global_mean)\n",
    "            \n",
    "            loss += batch_loss.item()\n",
    "            #metric += batch_metric.item()\n",
    "            \n",
    "        num_batches = len(dataloader)\n",
    "        loss = loss/num_batches\n",
    "        #metric = metric/num_batches\n",
    "        return loss#(loss, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f2f448-f056-43fa-a21a-52ae09d31f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device='cuda'\n",
    "from sklearn.metrics import r2_score\n",
    "n_epochs = 5000#5000\n",
    "device='cuda'\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\",50,100)\n",
    "    dropout_rates = trial.suggest_int(\"dropout_rates\",0,0.5)\n",
    "    model = MyLSTM(input_size=X_train.shape[-1],\\\n",
    "                   hidden_units=hidden_dim,\\\n",
    "                  dropout_rates=dropout_rates).to('cuda')\n",
    "    # Define Loss, Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = trial.suggest_loguniform(\"lr\",0.0001,0.001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    input_seq = torch.FloatTensor(X_train).to('cuda')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_NSEs = []\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "            output = model(features)\n",
    "            batch_loss = criterion(output,labels)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += batch_loss.item()\n",
    "        #output = output.to('cuda')\n",
    "        #target_seq = rfreg_funcs.flatten([storeYs['train'][key][str(targetname)] for key in storeYs['train'].keys()])\n",
    "        #target = torch.FloatTensor(target_seq).to('cuda')\n",
    "        #loss = criterion(output, torch.FloatTensor(np.asarray(target_seq)).to('cuda'))#.view(-1).long())        \n",
    "        #loss.backward() # Does backpropagation and calculates gradients\n",
    "        #optimizer.step() # Updates the weights accordingly\n",
    "        loss = loss/len(train_loader)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        val_loss = eval_model(model,\n",
    "                              valid_loader,\n",
    "                              criterion\n",
    "                             )\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        #val_NSEs.append(val_NSE)\n",
    "        if epoch%100 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss))\n",
    "            \n",
    "        if val_loss <= min(val_losses):\n",
    "            torch.save(model,'best_model'+str(trial.number)+'.'+str(targetname)+'.pt')\n",
    "            \n",
    "        # Validation loss\n",
    "        #val_seq = rfreg_funcs.flatten([storeYs['valid'][key][str(targetname)] for key in storeYs['valid'].keys()])\n",
    "        #val_loss = criterion(output, torch.FloatTensor(np.asarray(val_seq)).to('cuda'))\n",
    "        # early stopping\n",
    "        #early_stopping(loss, val_loss)\n",
    "        #if early_stopping.early_stop:\n",
    "            #continue\n",
    "        #    break\n",
    "    #targety = (rfreg_funcs.flatten([storeYs['newtest'][key][str(targetname)] \n",
    "    #                                                 for key in storeYs['newtest'].keys()]))\n",
    "    #targetx = torch.FloatTensor(X_newtest).to('cuda')\n",
    "    #pred = np.squeeze(model(targetx).cpu().detach().numpy().transpose())\n",
    "    #print(model(targetx).cpu().detach().numpy().transpose().squeeze())\n",
    "    rfreg_funcs.save_to_pickle('./tmp_rnn/'+str(targetname)+'/modeltrials_'+str(trial.number),model)\n",
    "    \n",
    "    return loss#loss#{'model':model,'trainloss':loss,'val_loss':val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67b16616",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 15:16:52,924]\u001b[0m A new study created in memory with name: no-name-6852db39-9042-4c27-92c3-bdda328a85b7\u001b[0m\n",
      "/tmp/26692921/ipykernel_1148060/91416613.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\",0.0001,0.001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/5000............. Loss: 0.1838\n",
      "Epoch: 200/5000............. Loss: 0.1375\n",
      "Epoch: 300/5000............. Loss: 0.1074\n",
      "Epoch: 400/5000............. Loss: 0.0953\n",
      "Epoch: 500/5000............. Loss: 0.1023\n",
      "Epoch: 600/5000............. Loss: 0.0802\n",
      "Epoch: 700/5000............. Loss: 0.0687\n",
      "Epoch: 800/5000............. Loss: 0.0639\n",
      "Epoch: 900/5000............. Loss: 0.0596\n",
      "Epoch: 1000/5000............. Loss: 0.0629\n",
      "Epoch: 1100/5000............. Loss: 0.0540\n",
      "Epoch: 1200/5000............. Loss: 0.0554\n",
      "Epoch: 1300/5000............. Loss: 0.0566\n",
      "Epoch: 1400/5000............. Loss: 0.0711\n",
      "Epoch: 1500/5000............. Loss: 0.0471\n",
      "Epoch: 1600/5000............. Loss: 0.0488\n",
      "Epoch: 1700/5000............. Loss: 0.0454\n",
      "Epoch: 1800/5000............. Loss: 0.0432\n",
      "Epoch: 1900/5000............. Loss: 0.0402\n",
      "Epoch: 2000/5000............. Loss: 0.0413\n",
      "Epoch: 2100/5000............. Loss: 0.0396\n",
      "Epoch: 2200/5000............. Loss: 0.0401\n",
      "Epoch: 2300/5000............. Loss: 0.0411\n",
      "Epoch: 2400/5000............. Loss: 0.0376\n",
      "Epoch: 2500/5000............. Loss: 0.0344\n",
      "Epoch: 2600/5000............. Loss: 0.0362\n",
      "Epoch: 2700/5000............. Loss: 0.0409\n",
      "Epoch: 2800/5000............. Loss: 0.0363\n",
      "Epoch: 2900/5000............. Loss: 0.0350\n",
      "Epoch: 3000/5000............. Loss: 0.0331\n",
      "Epoch: 3100/5000............. Loss: 0.0369\n",
      "Epoch: 3200/5000............. Loss: 0.0320\n",
      "Epoch: 3300/5000............. Loss: 0.0345\n",
      "Epoch: 3400/5000............. Loss: 0.0341\n",
      "Epoch: 3500/5000............. Loss: 0.0324\n",
      "Epoch: 3600/5000............. Loss: 0.0332\n",
      "Epoch: 3700/5000............. Loss: 0.0303\n",
      "Epoch: 3800/5000............. Loss: 0.0333\n",
      "Epoch: 3900/5000............. Loss: 0.0295\n",
      "Epoch: 4000/5000............. Loss: 0.0330\n",
      "Epoch: 4100/5000............. Loss: 0.0290\n",
      "Epoch: 4200/5000............. Loss: 0.0273\n",
      "Epoch: 4300/5000............. Loss: 0.0284\n",
      "Epoch: 4400/5000............. Loss: 0.0290\n",
      "Epoch: 4500/5000............. Loss: 0.0264\n",
      "Epoch: 4600/5000............. Loss: 0.0312\n",
      "Epoch: 4700/5000............. Loss: 0.0273\n",
      "Epoch: 4800/5000............. Loss: 0.0277\n",
      "Epoch: 4900/5000............. Loss: 0.0293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 15:26:56,082]\u001b[0m Trial 0 finished with value: 0.027316521693553242 and parameters: {'hidden_dim': 66, 'dropout_rates': 0, 'lr': 0.0005364346557792418}. Best is trial 0 with value: 0.027316521693553242.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0273\n",
      "Epoch: 100/5000............. Loss: 0.2230\n",
      "Epoch: 200/5000............. Loss: 0.1669\n",
      "Epoch: 300/5000............. Loss: 0.1375\n",
      "Epoch: 400/5000............. Loss: 0.1091\n",
      "Epoch: 500/5000............. Loss: 0.0912\n",
      "Epoch: 600/5000............. Loss: 0.0807\n",
      "Epoch: 700/5000............. Loss: 0.0708\n",
      "Epoch: 800/5000............. Loss: 0.0645\n",
      "Epoch: 900/5000............. Loss: 0.0602\n",
      "Epoch: 1000/5000............. Loss: 0.0571\n",
      "Epoch: 1100/5000............. Loss: 0.0514\n",
      "Epoch: 1200/5000............. Loss: 0.0514\n",
      "Epoch: 1300/5000............. Loss: 0.0466\n",
      "Epoch: 1400/5000............. Loss: 0.0438\n",
      "Epoch: 1500/5000............. Loss: 0.0408\n",
      "Epoch: 1600/5000............. Loss: 0.0398\n",
      "Epoch: 1700/5000............. Loss: 0.0436\n",
      "Epoch: 1800/5000............. Loss: 0.0371\n",
      "Epoch: 1900/5000............. Loss: 0.0372\n",
      "Epoch: 2000/5000............. Loss: 0.0342\n",
      "Epoch: 2100/5000............. Loss: 0.0330\n",
      "Epoch: 2200/5000............. Loss: 0.0330\n",
      "Epoch: 2300/5000............. Loss: 0.0375\n",
      "Epoch: 2400/5000............. Loss: 0.0324\n",
      "Epoch: 2500/5000............. Loss: 0.0296\n",
      "Epoch: 2600/5000............. Loss: 0.0295\n",
      "Epoch: 2700/5000............. Loss: 0.0273\n",
      "Epoch: 2800/5000............. Loss: 0.0264\n",
      "Epoch: 2900/5000............. Loss: 0.0275\n",
      "Epoch: 3000/5000............. Loss: 0.0269\n",
      "Epoch: 3100/5000............. Loss: 0.0258\n",
      "Epoch: 3200/5000............. Loss: 0.0245\n",
      "Epoch: 3300/5000............. Loss: 0.0248\n",
      "Epoch: 3400/5000............. Loss: 0.0232\n",
      "Epoch: 3500/5000............. Loss: 0.0228\n",
      "Epoch: 3600/5000............. Loss: 0.0256\n",
      "Epoch: 3700/5000............. Loss: 0.0239\n",
      "Epoch: 3800/5000............. Loss: 0.0239\n",
      "Epoch: 3900/5000............. Loss: 0.0220\n",
      "Epoch: 4000/5000............. Loss: 0.0205\n",
      "Epoch: 4100/5000............. Loss: 0.0208\n",
      "Epoch: 4200/5000............. Loss: 0.0206\n",
      "Epoch: 4300/5000............. Loss: 0.0204\n",
      "Epoch: 4400/5000............. Loss: 0.0213\n",
      "Epoch: 4500/5000............. Loss: 0.0204\n",
      "Epoch: 4600/5000............. Loss: 0.0227\n",
      "Epoch: 4700/5000............. Loss: 0.0225\n",
      "Epoch: 4800/5000............. Loss: 0.0193\n",
      "Epoch: 4900/5000............. Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 15:37:13,745]\u001b[0m Trial 1 finished with value: 0.017877290131790298 and parameters: {'hidden_dim': 57, 'dropout_rates': 0, 'lr': 0.00024927608237335926}. Best is trial 1 with value: 0.017877290131790298.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0179\n",
      "Epoch: 100/5000............. Loss: 0.2376\n",
      "Epoch: 200/5000............. Loss: 0.1886\n",
      "Epoch: 300/5000............. Loss: 0.1646\n",
      "Epoch: 400/5000............. Loss: 0.1516\n",
      "Epoch: 500/5000............. Loss: 0.1368\n",
      "Epoch: 600/5000............. Loss: 0.1341\n",
      "Epoch: 700/5000............. Loss: 0.1232\n",
      "Epoch: 800/5000............. Loss: 0.1164\n",
      "Epoch: 900/5000............. Loss: 0.1119\n",
      "Epoch: 1000/5000............. Loss: 0.1065\n",
      "Epoch: 1100/5000............. Loss: 0.0996\n",
      "Epoch: 1200/5000............. Loss: 0.0972\n",
      "Epoch: 1300/5000............. Loss: 0.0888\n",
      "Epoch: 1400/5000............. Loss: 0.0864\n",
      "Epoch: 1500/5000............. Loss: 0.0846\n",
      "Epoch: 1600/5000............. Loss: 0.0793\n",
      "Epoch: 1700/5000............. Loss: 0.0795\n",
      "Epoch: 1800/5000............. Loss: 0.0769\n",
      "Epoch: 1900/5000............. Loss: 0.0742\n",
      "Epoch: 2000/5000............. Loss: 0.0775\n",
      "Epoch: 2100/5000............. Loss: 0.0736\n",
      "Epoch: 2200/5000............. Loss: 0.0745\n",
      "Epoch: 2300/5000............. Loss: 0.0714\n",
      "Epoch: 2400/5000............. Loss: 0.0705\n",
      "Epoch: 2500/5000............. Loss: 0.0644\n",
      "Epoch: 2600/5000............. Loss: 0.0697\n",
      "Epoch: 2700/5000............. Loss: 0.0655\n",
      "Epoch: 2800/5000............. Loss: 0.0647\n",
      "Epoch: 2900/5000............. Loss: 0.0635\n",
      "Epoch: 3000/5000............. Loss: 0.0614\n",
      "Epoch: 3100/5000............. Loss: 0.0620\n",
      "Epoch: 3200/5000............. Loss: 0.0662\n",
      "Epoch: 3300/5000............. Loss: 0.0633\n",
      "Epoch: 3400/5000............. Loss: 0.0599\n",
      "Epoch: 3500/5000............. Loss: 0.0563\n",
      "Epoch: 3600/5000............. Loss: 0.0596\n",
      "Epoch: 3700/5000............. Loss: 0.0605\n",
      "Epoch: 3800/5000............. Loss: 0.0550\n",
      "Epoch: 3900/5000............. Loss: 0.0604\n",
      "Epoch: 4000/5000............. Loss: 0.0667\n",
      "Epoch: 4100/5000............. Loss: 0.0534\n",
      "Epoch: 4200/5000............. Loss: 0.0526\n",
      "Epoch: 4300/5000............. Loss: 0.0542\n",
      "Epoch: 4400/5000............. Loss: 0.0564\n",
      "Epoch: 4500/5000............. Loss: 0.0534\n",
      "Epoch: 4600/5000............. Loss: 0.0545\n",
      "Epoch: 4700/5000............. Loss: 0.0518\n",
      "Epoch: 4800/5000............. Loss: 0.0526\n",
      "Epoch: 4900/5000............. Loss: 0.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 15:47:16,017]\u001b[0m Trial 2 finished with value: 0.05241401248744556 and parameters: {'hidden_dim': 50, 'dropout_rates': 0, 'lr': 0.00042431694452035694}. Best is trial 1 with value: 0.017877290131790298.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0524\n",
      "Epoch: 100/5000............. Loss: 0.2033\n",
      "Epoch: 200/5000............. Loss: 0.1608\n",
      "Epoch: 300/5000............. Loss: 0.1331\n",
      "Epoch: 400/5000............. Loss: 0.1139\n",
      "Epoch: 500/5000............. Loss: 0.0997\n",
      "Epoch: 600/5000............. Loss: 0.0888\n",
      "Epoch: 700/5000............. Loss: 0.0870\n",
      "Epoch: 800/5000............. Loss: 0.0785\n",
      "Epoch: 900/5000............. Loss: 0.0734\n",
      "Epoch: 1000/5000............. Loss: 0.0691\n",
      "Epoch: 1100/5000............. Loss: 0.0696\n",
      "Epoch: 1200/5000............. Loss: 0.0627\n",
      "Epoch: 1300/5000............. Loss: 0.0642\n",
      "Epoch: 1400/5000............. Loss: 0.0573\n",
      "Epoch: 1500/5000............. Loss: 0.0584\n",
      "Epoch: 1600/5000............. Loss: 0.0554\n",
      "Epoch: 1700/5000............. Loss: 0.0534\n",
      "Epoch: 1800/5000............. Loss: 0.0557\n",
      "Epoch: 1900/5000............. Loss: 0.0507\n",
      "Epoch: 2000/5000............. Loss: 0.0513\n",
      "Epoch: 2100/5000............. Loss: 0.0502\n",
      "Epoch: 2200/5000............. Loss: 0.0489\n",
      "Epoch: 2300/5000............. Loss: 0.0500\n",
      "Epoch: 2400/5000............. Loss: 0.0463\n",
      "Epoch: 2500/5000............. Loss: 0.0456\n",
      "Epoch: 2600/5000............. Loss: 0.0499\n",
      "Epoch: 2700/5000............. Loss: 0.0442\n",
      "Epoch: 2800/5000............. Loss: 0.0433\n",
      "Epoch: 2900/5000............. Loss: 0.0476\n",
      "Epoch: 3000/5000............. Loss: 0.0448\n",
      "Epoch: 3100/5000............. Loss: 0.0442\n",
      "Epoch: 3200/5000............. Loss: 0.0449\n",
      "Epoch: 3300/5000............. Loss: 0.0508\n",
      "Epoch: 3400/5000............. Loss: 0.0444\n",
      "Epoch: 3500/5000............. Loss: 0.0437\n",
      "Epoch: 3600/5000............. Loss: 0.0372\n",
      "Epoch: 3700/5000............. Loss: 0.0386\n",
      "Epoch: 3800/5000............. Loss: 0.0464\n",
      "Epoch: 3900/5000............. Loss: 0.0399\n",
      "Epoch: 4000/5000............. Loss: 0.0472\n",
      "Epoch: 4100/5000............. Loss: 0.0393\n",
      "Epoch: 4200/5000............. Loss: 0.0428\n",
      "Epoch: 4300/5000............. Loss: 0.0358\n",
      "Epoch: 4400/5000............. Loss: 0.0367\n",
      "Epoch: 4500/5000............. Loss: 0.0375\n",
      "Epoch: 4600/5000............. Loss: 0.0357\n",
      "Epoch: 4700/5000............. Loss: 0.0384\n",
      "Epoch: 4800/5000............. Loss: 0.0360\n",
      "Epoch: 4900/5000............. Loss: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 15:57:28,110]\u001b[0m Trial 3 finished with value: 0.03951248776699815 and parameters: {'hidden_dim': 69, 'dropout_rates': 0, 'lr': 0.0009867716331653223}. Best is trial 1 with value: 0.017877290131790298.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0395\n",
      "Epoch: 100/5000............. Loss: 0.2283\n",
      "Epoch: 200/5000............. Loss: 0.1685\n",
      "Epoch: 300/5000............. Loss: 0.1334\n",
      "Epoch: 400/5000............. Loss: 0.1110\n",
      "Epoch: 500/5000............. Loss: 0.0961\n",
      "Epoch: 600/5000............. Loss: 0.0795\n",
      "Epoch: 700/5000............. Loss: 0.0721\n",
      "Epoch: 800/5000............. Loss: 0.0635\n",
      "Epoch: 900/5000............. Loss: 0.0556\n",
      "Epoch: 1000/5000............. Loss: 0.0540\n",
      "Epoch: 1100/5000............. Loss: 0.0469\n",
      "Epoch: 1200/5000............. Loss: 0.0439\n",
      "Epoch: 1300/5000............. Loss: 0.0399\n",
      "Epoch: 1400/5000............. Loss: 0.0394\n",
      "Epoch: 1500/5000............. Loss: 0.0376\n",
      "Epoch: 1600/5000............. Loss: 0.0361\n",
      "Epoch: 1700/5000............. Loss: 0.0314\n",
      "Epoch: 1800/5000............. Loss: 0.0322\n",
      "Epoch: 1900/5000............. Loss: 0.0290\n",
      "Epoch: 2000/5000............. Loss: 0.0342\n",
      "Epoch: 2100/5000............. Loss: 0.0294\n",
      "Epoch: 2200/5000............. Loss: 0.0256\n",
      "Epoch: 2300/5000............. Loss: 0.0244\n",
      "Epoch: 2400/5000............. Loss: 0.0235\n",
      "Epoch: 2500/5000............. Loss: 0.0227\n",
      "Epoch: 2600/5000............. Loss: 0.0250\n",
      "Epoch: 2700/5000............. Loss: 0.0217\n",
      "Epoch: 2800/5000............. Loss: 0.0232\n",
      "Epoch: 2900/5000............. Loss: 0.0212\n",
      "Epoch: 3000/5000............. Loss: 0.0215\n",
      "Epoch: 3100/5000............. Loss: 0.0193\n",
      "Epoch: 3200/5000............. Loss: 0.0191\n",
      "Epoch: 3300/5000............. Loss: 0.0181\n",
      "Epoch: 3400/5000............. Loss: 0.0175\n",
      "Epoch: 3500/5000............. Loss: 0.0176\n",
      "Epoch: 3600/5000............. Loss: 0.0187\n",
      "Epoch: 3700/5000............. Loss: 0.0170\n",
      "Epoch: 3800/5000............. Loss: 0.0167\n",
      "Epoch: 3900/5000............. Loss: 0.0167\n",
      "Epoch: 4000/5000............. Loss: 0.0159\n",
      "Epoch: 4100/5000............. Loss: 0.0154\n",
      "Epoch: 4200/5000............. Loss: 0.0150\n",
      "Epoch: 4300/5000............. Loss: 0.0159\n",
      "Epoch: 4400/5000............. Loss: 0.0144\n",
      "Epoch: 4500/5000............. Loss: 0.0140\n",
      "Epoch: 4600/5000............. Loss: 0.0146\n",
      "Epoch: 4700/5000............. Loss: 0.0157\n",
      "Epoch: 4800/5000............. Loss: 0.0140\n",
      "Epoch: 4900/5000............. Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:07:41,302]\u001b[0m Trial 4 finished with value: 0.012622481930468764 and parameters: {'hidden_dim': 77, 'dropout_rates': 0, 'lr': 0.00016094657799839727}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0126\n",
      "Epoch: 100/5000............. Loss: 0.2421\n",
      "Epoch: 200/5000............. Loss: 0.1794\n",
      "Epoch: 300/5000............. Loss: 0.1450\n",
      "Epoch: 400/5000............. Loss: 0.1159\n",
      "Epoch: 500/5000............. Loss: 0.1020\n",
      "Epoch: 600/5000............. Loss: 0.0864\n",
      "Epoch: 700/5000............. Loss: 0.0763\n",
      "Epoch: 800/5000............. Loss: 0.0693\n",
      "Epoch: 900/5000............. Loss: 0.0647\n",
      "Epoch: 1000/5000............. Loss: 0.0614\n",
      "Epoch: 1100/5000............. Loss: 0.0531\n",
      "Epoch: 1200/5000............. Loss: 0.0494\n",
      "Epoch: 1300/5000............. Loss: 0.0467\n",
      "Epoch: 1400/5000............. Loss: 0.0428\n",
      "Epoch: 1500/5000............. Loss: 0.0410\n",
      "Epoch: 1600/5000............. Loss: 0.0403\n",
      "Epoch: 1700/5000............. Loss: 0.0382\n",
      "Epoch: 1800/5000............. Loss: 0.0361\n",
      "Epoch: 1900/5000............. Loss: 0.0347\n",
      "Epoch: 2000/5000............. Loss: 0.0335\n",
      "Epoch: 2100/5000............. Loss: 0.0318\n",
      "Epoch: 2200/5000............. Loss: 0.0316\n",
      "Epoch: 2300/5000............. Loss: 0.0288\n",
      "Epoch: 2400/5000............. Loss: 0.0312\n",
      "Epoch: 2500/5000............. Loss: 0.0269\n",
      "Epoch: 2600/5000............. Loss: 0.0281\n",
      "Epoch: 2700/5000............. Loss: 0.0283\n",
      "Epoch: 2800/5000............. Loss: 0.0251\n",
      "Epoch: 2900/5000............. Loss: 0.0232\n",
      "Epoch: 3000/5000............. Loss: 0.0250\n",
      "Epoch: 3100/5000............. Loss: 0.0221\n",
      "Epoch: 3200/5000............. Loss: 0.0225\n",
      "Epoch: 3300/5000............. Loss: 0.0214\n",
      "Epoch: 3400/5000............. Loss: 0.0201\n",
      "Epoch: 3500/5000............. Loss: 0.0223\n",
      "Epoch: 3600/5000............. Loss: 0.0203\n",
      "Epoch: 3700/5000............. Loss: 0.0202\n",
      "Epoch: 3800/5000............. Loss: 0.0186\n",
      "Epoch: 3900/5000............. Loss: 0.0193\n",
      "Epoch: 4000/5000............. Loss: 0.0189\n",
      "Epoch: 4100/5000............. Loss: 0.0202\n",
      "Epoch: 4200/5000............. Loss: 0.0169\n",
      "Epoch: 4300/5000............. Loss: 0.0170\n",
      "Epoch: 4400/5000............. Loss: 0.0176\n",
      "Epoch: 4500/5000............. Loss: 0.0165\n",
      "Epoch: 4600/5000............. Loss: 0.0171\n",
      "Epoch: 4700/5000............. Loss: 0.0156\n",
      "Epoch: 4800/5000............. Loss: 0.0174\n",
      "Epoch: 4900/5000............. Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:17:55,350]\u001b[0m Trial 5 finished with value: 0.016421036102942058 and parameters: {'hidden_dim': 71, 'dropout_rates': 0, 'lr': 0.0001846360828269591}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0164\n",
      "Epoch: 100/5000............. Loss: 0.2398\n",
      "Epoch: 200/5000............. Loss: 0.1765\n",
      "Epoch: 300/5000............. Loss: 0.1391\n",
      "Epoch: 400/5000............. Loss: 0.1164\n",
      "Epoch: 500/5000............. Loss: 0.1005\n",
      "Epoch: 600/5000............. Loss: 0.0848\n",
      "Epoch: 700/5000............. Loss: 0.0753\n",
      "Epoch: 800/5000............. Loss: 0.0696\n",
      "Epoch: 900/5000............. Loss: 0.0601\n",
      "Epoch: 1000/5000............. Loss: 0.0551\n",
      "Epoch: 1100/5000............. Loss: 0.0522\n",
      "Epoch: 1200/5000............. Loss: 0.0476\n",
      "Epoch: 1300/5000............. Loss: 0.0450\n",
      "Epoch: 1400/5000............. Loss: 0.0412\n",
      "Epoch: 1500/5000............. Loss: 0.0389\n",
      "Epoch: 1600/5000............. Loss: 0.0367\n",
      "Epoch: 1700/5000............. Loss: 0.0365\n",
      "Epoch: 1800/5000............. Loss: 0.0340\n",
      "Epoch: 1900/5000............. Loss: 0.0334\n",
      "Epoch: 2000/5000............. Loss: 0.0311\n",
      "Epoch: 2100/5000............. Loss: 0.0301\n",
      "Epoch: 2200/5000............. Loss: 0.0280\n",
      "Epoch: 2300/5000............. Loss: 0.0267\n",
      "Epoch: 2400/5000............. Loss: 0.0268\n",
      "Epoch: 2500/5000............. Loss: 0.0261\n",
      "Epoch: 2600/5000............. Loss: 0.0242\n",
      "Epoch: 2700/5000............. Loss: 0.0250\n",
      "Epoch: 2800/5000............. Loss: 0.0233\n",
      "Epoch: 2900/5000............. Loss: 0.0258\n",
      "Epoch: 3000/5000............. Loss: 0.0221\n",
      "Epoch: 3100/5000............. Loss: 0.0227\n",
      "Epoch: 3200/5000............. Loss: 0.0207\n",
      "Epoch: 3300/5000............. Loss: 0.0209\n",
      "Epoch: 3400/5000............. Loss: 0.0193\n",
      "Epoch: 3500/5000............. Loss: 0.0198\n",
      "Epoch: 3600/5000............. Loss: 0.0189\n",
      "Epoch: 3700/5000............. Loss: 0.0197\n",
      "Epoch: 3800/5000............. Loss: 0.0195\n",
      "Epoch: 3900/5000............. Loss: 0.0180\n",
      "Epoch: 4000/5000............. Loss: 0.0185\n",
      "Epoch: 4100/5000............. Loss: 0.0174\n",
      "Epoch: 4200/5000............. Loss: 0.0171\n",
      "Epoch: 4300/5000............. Loss: 0.0198\n",
      "Epoch: 4400/5000............. Loss: 0.0179\n",
      "Epoch: 4500/5000............. Loss: 0.0159\n",
      "Epoch: 4600/5000............. Loss: 0.0156\n",
      "Epoch: 4700/5000............. Loss: 0.0149\n",
      "Epoch: 4800/5000............. Loss: 0.0162\n",
      "Epoch: 4900/5000............. Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:28:04,444]\u001b[0m Trial 6 finished with value: 0.014857491478323936 and parameters: {'hidden_dim': 70, 'dropout_rates': 0, 'lr': 0.00011461806271468013}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0149\n",
      "Epoch: 100/5000............. Loss: 0.2547\n",
      "Epoch: 200/5000............. Loss: 0.2004\n",
      "Epoch: 300/5000............. Loss: 0.1616\n",
      "Epoch: 400/5000............. Loss: 0.1410\n",
      "Epoch: 500/5000............. Loss: 0.1197\n",
      "Epoch: 600/5000............. Loss: 0.1051\n",
      "Epoch: 700/5000............. Loss: 0.0972\n",
      "Epoch: 800/5000............. Loss: 0.0848\n",
      "Epoch: 900/5000............. Loss: 0.0772\n",
      "Epoch: 1000/5000............. Loss: 0.0709\n",
      "Epoch: 1100/5000............. Loss: 0.0676\n",
      "Epoch: 1200/5000............. Loss: 0.0600\n",
      "Epoch: 1300/5000............. Loss: 0.0568\n",
      "Epoch: 1400/5000............. Loss: 0.0531\n",
      "Epoch: 1500/5000............. Loss: 0.0516\n",
      "Epoch: 1600/5000............. Loss: 0.0478\n",
      "Epoch: 1700/5000............. Loss: 0.0461\n",
      "Epoch: 1800/5000............. Loss: 0.0449\n",
      "Epoch: 1900/5000............. Loss: 0.0419\n",
      "Epoch: 2000/5000............. Loss: 0.0403\n",
      "Epoch: 2100/5000............. Loss: 0.0393\n",
      "Epoch: 2200/5000............. Loss: 0.0385\n",
      "Epoch: 2300/5000............. Loss: 0.0374\n",
      "Epoch: 2400/5000............. Loss: 0.0347\n",
      "Epoch: 2500/5000............. Loss: 0.0344\n",
      "Epoch: 2600/5000............. Loss: 0.0320\n",
      "Epoch: 2700/5000............. Loss: 0.0313\n",
      "Epoch: 2800/5000............. Loss: 0.0320\n",
      "Epoch: 2900/5000............. Loss: 0.0317\n",
      "Epoch: 3000/5000............. Loss: 0.0287\n",
      "Epoch: 3100/5000............. Loss: 0.0291\n",
      "Epoch: 3200/5000............. Loss: 0.0307\n",
      "Epoch: 3300/5000............. Loss: 0.0276\n",
      "Epoch: 3400/5000............. Loss: 0.0276\n",
      "Epoch: 3500/5000............. Loss: 0.0270\n",
      "Epoch: 3600/5000............. Loss: 0.0277\n",
      "Epoch: 3700/5000............. Loss: 0.0298\n",
      "Epoch: 3800/5000............. Loss: 0.0254\n",
      "Epoch: 3900/5000............. Loss: 0.0266\n",
      "Epoch: 4000/5000............. Loss: 0.0273\n",
      "Epoch: 4100/5000............. Loss: 0.0263\n",
      "Epoch: 4200/5000............. Loss: 0.0231\n",
      "Epoch: 4300/5000............. Loss: 0.0244\n",
      "Epoch: 4400/5000............. Loss: 0.0230\n",
      "Epoch: 4500/5000............. Loss: 0.0237\n",
      "Epoch: 4600/5000............. Loss: 0.0242\n",
      "Epoch: 4700/5000............. Loss: 0.0246\n",
      "Epoch: 4800/5000............. Loss: 0.0218\n",
      "Epoch: 4900/5000............. Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:38:14,723]\u001b[0m Trial 7 finished with value: 0.02173343018761703 and parameters: {'hidden_dim': 62, 'dropout_rates': 0, 'lr': 0.00015881358638940483}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0217\n",
      "Epoch: 100/5000............. Loss: 0.2760\n",
      "Epoch: 200/5000............. Loss: 0.2126\n",
      "Epoch: 300/5000............. Loss: 0.1730\n",
      "Epoch: 400/5000............. Loss: 0.1438\n",
      "Epoch: 500/5000............. Loss: 0.1249\n",
      "Epoch: 600/5000............. Loss: 0.1119\n",
      "Epoch: 700/5000............. Loss: 0.1019\n",
      "Epoch: 800/5000............. Loss: 0.0908\n",
      "Epoch: 900/5000............. Loss: 0.0863\n",
      "Epoch: 1000/5000............. Loss: 0.0767\n",
      "Epoch: 1100/5000............. Loss: 0.0711\n",
      "Epoch: 1200/5000............. Loss: 0.0660\n",
      "Epoch: 1300/5000............. Loss: 0.0646\n",
      "Epoch: 1400/5000............. Loss: 0.0627\n",
      "Epoch: 1500/5000............. Loss: 0.0574\n",
      "Epoch: 1600/5000............. Loss: 0.0556\n",
      "Epoch: 1700/5000............. Loss: 0.0519\n",
      "Epoch: 1800/5000............. Loss: 0.0497\n",
      "Epoch: 1900/5000............. Loss: 0.0488\n",
      "Epoch: 2000/5000............. Loss: 0.0488\n",
      "Epoch: 2100/5000............. Loss: 0.0458\n",
      "Epoch: 2200/5000............. Loss: 0.0450\n",
      "Epoch: 2300/5000............. Loss: 0.0405\n",
      "Epoch: 2400/5000............. Loss: 0.0428\n",
      "Epoch: 2500/5000............. Loss: 0.0401\n",
      "Epoch: 2600/5000............. Loss: 0.0409\n",
      "Epoch: 2700/5000............. Loss: 0.0381\n",
      "Epoch: 2800/5000............. Loss: 0.0360\n",
      "Epoch: 2900/5000............. Loss: 0.0366\n",
      "Epoch: 3000/5000............. Loss: 0.0363\n",
      "Epoch: 3100/5000............. Loss: 0.0334\n",
      "Epoch: 3200/5000............. Loss: 0.0340\n",
      "Epoch: 3300/5000............. Loss: 0.0325\n",
      "Epoch: 3400/5000............. Loss: 0.0331\n",
      "Epoch: 3500/5000............. Loss: 0.0310\n",
      "Epoch: 3600/5000............. Loss: 0.0296\n",
      "Epoch: 3700/5000............. Loss: 0.0284\n",
      "Epoch: 3800/5000............. Loss: 0.0302\n",
      "Epoch: 3900/5000............. Loss: 0.0303\n",
      "Epoch: 4000/5000............. Loss: 0.0275\n",
      "Epoch: 4100/5000............. Loss: 0.0264\n",
      "Epoch: 4200/5000............. Loss: 0.0274\n",
      "Epoch: 4300/5000............. Loss: 0.0310\n",
      "Epoch: 4400/5000............. Loss: 0.0267\n",
      "Epoch: 4500/5000............. Loss: 0.0285\n",
      "Epoch: 4600/5000............. Loss: 0.0255\n",
      "Epoch: 4700/5000............. Loss: 0.0242\n",
      "Epoch: 4800/5000............. Loss: 0.0257\n",
      "Epoch: 4900/5000............. Loss: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:48:30,063]\u001b[0m Trial 8 finished with value: 0.024925301649740765 and parameters: {'hidden_dim': 58, 'dropout_rates': 0, 'lr': 0.0001493830924098759}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0249\n",
      "Epoch: 100/5000............. Loss: 0.2093\n",
      "Epoch: 200/5000............. Loss: 0.1477\n",
      "Epoch: 300/5000............. Loss: 0.1222\n",
      "Epoch: 400/5000............. Loss: 0.1394\n",
      "Epoch: 500/5000............. Loss: 0.0936\n",
      "Epoch: 600/5000............. Loss: 0.0798\n",
      "Epoch: 700/5000............. Loss: 0.0716\n",
      "Epoch: 800/5000............. Loss: 0.0622\n",
      "Epoch: 900/5000............. Loss: 0.0567\n",
      "Epoch: 1000/5000............. Loss: 0.0531\n",
      "Epoch: 1100/5000............. Loss: 0.0522\n",
      "Epoch: 1200/5000............. Loss: 0.0500\n",
      "Epoch: 1300/5000............. Loss: 0.0464\n",
      "Epoch: 1400/5000............. Loss: 0.0488\n",
      "Epoch: 1500/5000............. Loss: 0.0417\n",
      "Epoch: 1600/5000............. Loss: 0.0414\n",
      "Epoch: 1700/5000............. Loss: 0.0402\n",
      "Epoch: 1800/5000............. Loss: 0.0397\n",
      "Epoch: 1900/5000............. Loss: 0.0363\n",
      "Epoch: 2000/5000............. Loss: 0.0379\n",
      "Epoch: 2100/5000............. Loss: 0.0410\n",
      "Epoch: 2200/5000............. Loss: 0.0369\n",
      "Epoch: 2300/5000............. Loss: 0.0323\n",
      "Epoch: 2400/5000............. Loss: 0.0299\n",
      "Epoch: 2500/5000............. Loss: 0.0314\n",
      "Epoch: 2600/5000............. Loss: 0.0342\n",
      "Epoch: 2700/5000............. Loss: 0.0302\n",
      "Epoch: 2800/5000............. Loss: 0.0313\n",
      "Epoch: 2900/5000............. Loss: 0.0268\n",
      "Epoch: 3000/5000............. Loss: 0.0286\n",
      "Epoch: 3100/5000............. Loss: 0.0301\n",
      "Epoch: 3200/5000............. Loss: 0.0329\n",
      "Epoch: 3300/5000............. Loss: 0.0259\n",
      "Epoch: 3400/5000............. Loss: 0.0270\n",
      "Epoch: 3500/5000............. Loss: 0.0248\n",
      "Epoch: 3600/5000............. Loss: 0.0242\n",
      "Epoch: 3700/5000............. Loss: 0.0247\n",
      "Epoch: 3800/5000............. Loss: 0.0252\n",
      "Epoch: 3900/5000............. Loss: 0.0256\n",
      "Epoch: 4000/5000............. Loss: 0.0244\n",
      "Epoch: 4100/5000............. Loss: 0.0233\n",
      "Epoch: 4200/5000............. Loss: 0.0229\n",
      "Epoch: 4300/5000............. Loss: 0.0252\n",
      "Epoch: 4400/5000............. Loss: 0.0219\n",
      "Epoch: 4500/5000............. Loss: 0.0238\n",
      "Epoch: 4600/5000............. Loss: 0.0241\n",
      "Epoch: 4700/5000............. Loss: 0.0217\n",
      "Epoch: 4800/5000............. Loss: 0.0220\n",
      "Epoch: 4900/5000............. Loss: 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-27 16:58:38,012]\u001b[0m Trial 9 finished with value: 0.02115373281495912 and parameters: {'hidden_dim': 69, 'dropout_rates': 0, 'lr': 0.0003053860929538442}. Best is trial 4 with value: 0.012622481930468764.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/5000............. Loss: 0.0212\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(directions=[\"minimize\"])\n",
    "study.optimize(objective, n_trials=10)#, timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c90c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg_funcs.save_to_pickle('./tmp_rnn/'+str(targetname)+'/studyinfo_pmin.pkl',study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a08e1d",
   "metadata": {},
   "source": [
    "## Without optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e0a8252",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m targetname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X_train)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDa\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3300\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clears existing gradients from previous epoch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Da' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "device='cuda'\n",
    "criterion = nn.MSELoss()\n",
    "model = RNNModel2(input_size=X_train.shape[-1], \n",
    "                  output_size=1, \n",
    "                  hidden_dim=10, \n",
    "                  n_layers=1).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "targetname = 'pmin'\n",
    "input_seq = torch.FloatTensor(X_train).to('cuda')\n",
    "train_loader = Da\n",
    "for epoch in range(1, 3300 + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    #output = output.to('cuda')\n",
    "    \n",
    "    target_seq = rfreg_funcs.flatten([storeYs['train'][key][str(targetname)] for key in storeYs['train'].keys()])\n",
    "    target = torch.FloatTensor(target_seq).to('cuda')\n",
    "    loss = criterion(output, torch.FloatTensor(np.asarray(target_seq)).to('cuda'))#.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a474b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel2(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        #out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(hidden[0])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to('cuda')\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
