{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4dd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8f7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/Data/ECMWF/ERA5_25kmx3hr/\"\n",
    "path=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/wp/\"\n",
    "output=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/create_ts/outputs/\"\n",
    "target=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/create_ts/outputs/targets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1f9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = sorted(glob.glob(path+'wp_2015.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a165ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksDF = pd.read_csv(track[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8827a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksDF['name'].unique()\n",
    "stormnames = list(tracksDF['name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ad1858",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015-01-12',\n",
       " '2015-01-12',\n",
       " '2015-01-12',\n",
       " '2015-01-12',\n",
       " '2015-01-12',\n",
       " '2015-01-12',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-13',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-14',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-15',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-16',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-17',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-18',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-19',\n",
       " '2015-01-20',\n",
       " '2015-01-20',\n",
       " '2015-01-20',\n",
       " '2015-01-20',\n",
       " '2015-01-20',\n",
       " '2015-01-20',\n",
       " '2015-01-20']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(tracksDF[tracksDF['name']==stormnames[0]].time[i]).split(':')[0] for i in range(len(tracksDF[tracksDF['name']==stormnames[0]].time))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e9db457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000_200, 1000_300, 1000_500, 1000_700, 1000_850, 850_200, 850_250, 850_300, 850_500, 925_200, 925_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3215057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e988cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssingle level variables\n",
    "def output_indices(TCtrack=None,ERA5date=None,ERA5hour=None):\n",
    "    allindices = []\n",
    "    for timeidx in range(len(TCtrack)):#len(track['time'])):\n",
    "        datetrack,hourtrack = TCtrack['time'][timeidx].split(':')[0],TCtrack['time'][timeidx].split(':')[1][0:2]\n",
    "        ####################################################################################################\n",
    "        # Find the indices in ERA5 data with the same date as track\n",
    "        ####################################################################################################\n",
    "        dateind = []\n",
    "        for ind,obj in enumerate(ERA5date):\n",
    "            if obj==datetrack:\n",
    "                dateind.append(ind)\n",
    "        del ind,obj\n",
    "        hourind = []\n",
    "        hourextract = ERA5hour[int(np.min(np.asarray(dateind))):int(np.max(np.asarray(dateind)))+1]\n",
    "        for ind,obj in enumerate(hourextract):\n",
    "            if obj==hourtrack:            \n",
    "                hourind.append(ind)\n",
    "        allindices.append((int(np.min(np.asarray(dateind))),int(hourind[0])))\n",
    "    return allindices\n",
    "\n",
    "def extract_var(dataset=None,var='var138',indices=None):\n",
    "    extractedvar = []\n",
    "    for i in (range(len(indices))):\n",
    "        realindex = indices[i][0]+indices[i][1]\n",
    "        extractedvar.append(dataset[var][int(realindex),...].data)\n",
    "    return np.asarray(extractedvar)\n",
    "\n",
    "def largearea(dataset=None,invar=None,indices=None):\n",
    "    if len(invar.shape) != 3:\n",
    "        invar = np.squeeze(invar)\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices)))),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    \n",
    "    var_out=np.zeros((len(indices),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        try:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "        except:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out\n",
    "\n",
    "def largearea_withpres(dataset=None,invar=None,indices=None):\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"plev\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices))),\n",
    "               plev=([\"plev\"],dataset.plev.data)),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    var_out=np.zeros((len(indices),len(dm2.plev.data),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        for ip in range(len(dm2.plev.data)):\n",
    "            try:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "            except:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731e8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Shear vars\n",
    "def createmask(dm=None,irad=None,orad=None,lonselect=None,latselect=None):\n",
    "    mask = []\n",
    "    for ti in range(len(tc_irad)):\n",
    "        lonselect = dm.lon.sel(lon=slice(orad[ti,:][2],orad[ti,:][3])).data\n",
    "        latselect = np.flipud(dm.lat.sel(lat=slice(orad[ti,:][1],orad[ti,:][0])).data)\n",
    "        if (lonselect.shape != 64) or (latselect.shape != 64):\n",
    "            lon2d,lat2d = np.meshgrid(lonselect[0:64],latselect[0:64])\n",
    "        else:\n",
    "            lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        #############################################################################################\n",
    "        latcriteria = np.logical_and(lat2d>irad[ti][0],lat2d<irad[ti][1])\n",
    "        loncriteria = np.logical_and(lon2d>irad[ti][2],lon2d<irad[ti][3])\n",
    "        allcriteria = np.logical_and(loncriteria,latcriteria)\n",
    "        mask.append(allcriteria)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678af7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyear(year=None,sheartype=None):\n",
    "    dm2 = xr.open_dataset(datapath+'/shear/shear_'+str(sheartype)+'_'+str(year)+'.nc')\n",
    "    #tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/wp/*_'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,era5_date,era5_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260a013b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f80f30daee4958b027d43649467de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb7d9b6ddb442898233a40426dd44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccf55d35f764c1d99156265b7f366ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab28a1d7cec9426384f30e111df94302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fa09f1a75b481fb719097e745e14c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f21f7c5d634b0fb0f51da4a85a5998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d5e1be24314f849bba2aeca4ebc3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6aa93165b247139629445a5432f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02ca45c63074df192ad01ccc2342c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c130ffe8f451465dbd619b41071ce537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15023f42d1bc4c49add8326b975a6b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCshears = {}\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    dm2,era5_date,era5_hour = readyear(2015,shear)\n",
    "    TCtempstore = []\n",
    "    for TCobj in tqdm(stormnames):\n",
    "        track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "        lon1=track['lon'].to_numpy()\n",
    "        lat1=track['lat'].to_numpy()\n",
    "        lonx=np.mod(lon1,360)\n",
    "        pos = arr = np.stack((lat1, lonx), axis=1)\n",
    "        ###########################################################################\n",
    "        indices_store = output_indices(track,era5_date,era5_hour)\n",
    "        ###########################################################################\n",
    "        mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "        ###########################################################################\n",
    "        tc_irad=np.empty((len(indices_store),4))\n",
    "        tc_irad[:,0] = pos[:,0]-2\n",
    "        tc_irad[:,1] = pos[:,0]+2\n",
    "        tc_irad[:,2] = pos[:,1]-2\n",
    "        tc_irad[:,3] = pos[:,1]+2\n",
    "        \n",
    "        tc_orad=np.empty((len(indices_store),4))\n",
    "        tc_orad[:,0] = pos[:,0]-8\n",
    "        tc_orad[:,1] = pos[:,0]+8\n",
    "        tc_orad[:,2] = pos[:,1]-8\n",
    "        tc_orad[:,3] = pos[:,1]+8\n",
    "        ###########################################################################\n",
    "        smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "        \n",
    "        svarname1 = ['shear_'+shear]#['shear_850_200']\n",
    "        svarname = ['shear']\n",
    "        \n",
    "        svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "        lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "        latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "        lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        \n",
    "        mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "        #############################################################################################\n",
    "        tsdict = {}\n",
    "        for ind,obj in (enumerate(svarname)):\n",
    "            tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "        #############################################################################################\n",
    "        TCtempstore.append(tsdict)\n",
    "    TCshears[shear] = TCtempstore\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7736d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTCshear1 = {}\n",
    "smallTCshear2 = {}\n",
    "smallTCshear3 = {}\n",
    "smallTCshear4 = {}\n",
    "smallTCshear5 = {}\n",
    "smallTCshear6 = {}\n",
    "smallTCshear7 = {}\n",
    "smallTCshear8 = {}\n",
    "smallTCshear9 = {}\n",
    "smallTCshear10 = {}\n",
    "smallTCshear11 = {}\n",
    "smallTCshear12 = {}\n",
    "smallTCshear13 = {}\n",
    "smallTCshear14 = {}\n",
    "smallTCshear15 = {}\n",
    "smallTCshear16 = {}\n",
    "smallTCshear17 = {}\n",
    "smallTCshear18 = {}\n",
    "#smallTCshear19 = {}\n",
    "#smallTCshear20 = {}\n",
    "#smallTCshear21 = {}\n",
    "#smallTCshear22 = {}\n",
    "#smallTCshear23 = {}\n",
    "#smallTCshear24 = {}\n",
    "\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    smallTCshear1['shear_'+str(shear)] = (TCshears[shear][0]['shear_'+str(shear)])\n",
    "    smallTCshear2['shear_'+str(shear)] = (TCshears[shear][1]['shear_'+str(shear)])\n",
    "    smallTCshear3['shear_'+str(shear)] = (TCshears[shear][2]['shear_'+str(shear)])\n",
    "    smallTCshear4['shear_'+str(shear)] = (TCshears[shear][3]['shear_'+str(shear)])\n",
    "    smallTCshear5['shear_'+str(shear)] = (TCshears[shear][4]['shear_'+str(shear)])\n",
    "    smallTCshear6['shear_'+str(shear)] = (TCshears[shear][5]['shear_'+str(shear)])\n",
    "    smallTCshear7['shear_'+str(shear)] = (TCshears[shear][6]['shear_'+str(shear)])\n",
    "    smallTCshear8['shear_'+str(shear)] = (TCshears[shear][7]['shear_'+str(shear)])\n",
    "    smallTCshear9['shear_'+str(shear)] = (TCshears[shear][8]['shear_'+str(shear)])\n",
    "    smallTCshear10['shear_'+str(shear)] = (TCshears[shear][9]['shear_'+str(shear)])\n",
    "    smallTCshear11['shear_'+str(shear)] = (TCshears[shear][10]['shear_'+str(shear)])\n",
    "    smallTCshear12['shear_'+str(shear)] = (TCshears[shear][11]['shear_'+str(shear)])\n",
    "    smallTCshear13['shear_'+str(shear)] = (TCshears[shear][12]['shear_'+str(shear)])\n",
    "    smallTCshear14['shear_'+str(shear)] = (TCshears[shear][13]['shear_'+str(shear)])\n",
    "    smallTCshear15['shear_'+str(shear)] = (TCshears[shear][14]['shear_'+str(shear)])\n",
    "    smallTCshear16['shear_'+str(shear)] = (TCshears[shear][15]['shear_'+str(shear)])\n",
    "    smallTCshear17['shear_'+str(shear)] = (TCshears[shear][16]['shear_'+str(shear)])\n",
    "    smallTCshear18['shear_'+str(shear)] = (TCshears[shear][17]['shear_'+str(shear)])\n",
    "    #smallTCshear19['shear_'+str(shear)] = (TCshears[shear][18]['shear_'+str(shear)])\n",
    "    #smallTCshear20['shear_'+str(shear)] = (TCshears[shear][19]['shear_'+str(shear)])\n",
    "    #smallTCshear21['shear_'+str(shear)] = (TCshears[shear][20]['shear_'+str(shear)])\n",
    "    #smallTCshear22['shear_'+str(shear)] = (TCshears[shear][21]['shear_'+str(shear)])\n",
    "    #smallTCshear23['shear_'+str(shear)] = (TCshears[shear][22]['shear_'+str(shear)])\n",
    "    #smallTCshear24['shear_'+str(shear)] = (TCshears[shear][23]['shear_'+str(shear)])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4bb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1=pd.DataFrame.from_dict(smallTCshear1)\n",
    "shears2=pd.DataFrame.from_dict(smallTCshear2)\n",
    "shears3=pd.DataFrame.from_dict(smallTCshear3)\n",
    "shears4=pd.DataFrame.from_dict(smallTCshear4)\n",
    "shears5=pd.DataFrame.from_dict(smallTCshear5)\n",
    "shears6=pd.DataFrame.from_dict(smallTCshear6)\n",
    "shears7=pd.DataFrame.from_dict(smallTCshear7)\n",
    "shears8=pd.DataFrame.from_dict(smallTCshear8)\n",
    "shears9=pd.DataFrame.from_dict(smallTCshear9)\n",
    "shears10=pd.DataFrame.from_dict(smallTCshear10)\n",
    "shears11=pd.DataFrame.from_dict(smallTCshear11)\n",
    "shears12=pd.DataFrame.from_dict(smallTCshear12)\n",
    "shears13=pd.DataFrame.from_dict(smallTCshear13)\n",
    "shears14=pd.DataFrame.from_dict(smallTCshear14)\n",
    "shears15=pd.DataFrame.from_dict(smallTCshear15)\n",
    "shears16=pd.DataFrame.from_dict(smallTCshear16)\n",
    "shears17=pd.DataFrame.from_dict(smallTCshear17)\n",
    "shears18=pd.DataFrame.from_dict(smallTCshear18)\n",
    "#shears19=pd.DataFrame.from_dict(smallTCshear19)\n",
    "#shears20=pd.DataFrame.from_dict(smallTCshear20)\n",
    "#shears21=pd.DataFrame.from_dict(smallTCshear21)\n",
    "#shears22=pd.DataFrame.from_dict(smallTCshear22)\n",
    "#shears23=pd.DataFrame.from_dict(smallTCshear23)\n",
    "#shears24=pd.DataFrame.from_dict(smallTCshear24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9516b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MEKKHALA',\n",
       " 'BAVI',\n",
       " 'NOUL',\n",
       " 'DOLPHIN',\n",
       " 'CHAN-HOM',\n",
       " 'LINFA',\n",
       " 'NANGKA',\n",
       " 'HALOLA',\n",
       " 'SOUDELOR',\n",
       " 'MOLAVE',\n",
       " 'GONI',\n",
       " 'ATSANI',\n",
       " 'KILO',\n",
       " 'KROVANH',\n",
       " 'DUJUAN',\n",
       " 'CHOI-WAN',\n",
       " 'CHAMPI',\n",
       " 'MELOR']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stormnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f67cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62b0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1.to_csv(output+'shears_wpouter_2015_MEKKHALA.csv')\n",
    "shears2.to_csv(output+'shears_wpouter_2015_BAVI.csv')\n",
    "shears3.to_csv(output+'shears_wpouter_2015_NOUL.csv')\n",
    "shears4.to_csv(output+'shears_wpouter_2015_DOLPHIN.csv')\n",
    "shears5.to_csv(output+'shears_wpouter_2015_CHAN-HOM.csv')\n",
    "shears6.to_csv(output+'shears_wpouter_2015_LINFA.csv')\n",
    "shears7.to_csv(output+'shears_wpouter_2015_NANGKA.csv')\n",
    "shears8.to_csv(output+'shears_wpouter_2015_HALOLA.csv')\n",
    "shears9.to_csv(output+'shears_wpouter_2015_SOUDELOR.csv')\n",
    "shears10.to_csv(output+'shears_wpouter_2015_MOLAVE.csv')\n",
    "shears11.to_csv(output+'shears_wpouter_2015_GONI.csv')\n",
    "shears12.to_csv(output+'shears_wpouter_2015_ATSANI.csv')\n",
    "shears13.to_csv(output+'shears_wpouter_2015_KILO.csv')\n",
    "shears14.to_csv(output+'shears_wpouter_2015_KROVANH.csv')\n",
    "shears15.to_csv(output+'shears_wpouter_2015_DUJUAN.csv')\n",
    "shears16.to_csv(output+'shears_wpouter_2015_CHOI-WAN.csv')\n",
    "shears17.to_csv(output+'shears_wpouter_2015_CHAMPI.csv')\n",
    "shears18.to_csv(output+'shears_wpouter_2015_MELOR.csv')\n",
    "#shears19.to_csv(output+'shears_wpouter_2015_MANGKHUT.csv')\n",
    "#shears20.to_csv(output+'shears_wpouter_2015_TRAMI.csv')\n",
    "#shears21.to_csv(output+'shears_wpouter_2015_USAGI.csv')\n",
    "#shears22.to_csv(output+'shears_wpouter_2015_MAN-YI.csv')\n",
    "#shears23.to_csv(output+'shears_wpouter_2015_TALAS.csv')\n",
    "#shears24.to_csv(output+'shears_wpouter_2015_NORU.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14245d",
   "metadata": {},
   "source": [
    "script ends here..!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65117b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6642cedd-e1da-461c-b2ad-720975aa3133",
   "metadata": {},
   "source": [
    "Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f27c2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = xr.open_dataset(datapath+'/div/div_2021.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d4d79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112de72cd03e4946b746a7e276ed1aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pressure vars - divergence3.nc')\n",
    "TCdiv_ts = []\n",
    "for TCobj in tqdm(stormnames):\n",
    "    track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lonx=np.mod(lon1,360)\n",
    "    pos = arr = np.stack((lat1, lonx), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mypvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallpvarout = [largearea_withpres(dm2,mypvar[i],indices_store) for i in (range(len(mypvar)))]    \n",
    "    pvarname1 = ['outdiv']\n",
    "    pvarname = ['var155']\n",
    "    pvardict = {varnameobj:varobj for (varnameobj,varobj) in zip(pvarname,smallpvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    #latselect = dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    ts_pdict = {}\n",
    "    for ind,obj in (enumerate(pvarname)):\n",
    "        pvarTS_store = []\n",
    "        for plevv in range(len(dm2.plev.data)):\n",
    "            tempvar = pvardict[pvarname[ind]][:,plevv,...]\n",
    "            tempts = [tempvar[i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tempTSERIES = [np.nanmean(obj) for obj in tempts]\n",
    "            pvarTS_store.append(tempTSERIES)\n",
    "        ts_pdict[pvarname1[ind]] = np.asarray(pvarTS_store).transpose()\n",
    "    #############################################################################################\n",
    "    TCdiv_ts.append(ts_pdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "744b29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plv = [str(int(obj)) for obj in dm2.plev.data/100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5614dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pandadict(divdict=None,divlv=plv):\n",
    "    tempdivdict = {'outdiv_'+divlv[i]:divdict['outdiv'][:,i] for i in range(divdict['outdiv'].shape[1])}\n",
    "    alldict = {**tempdivdict}\n",
    "    return alldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "40593893",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTCdicts = {}\n",
    "for ind in range(len(TCdiv_ts)):\n",
    "    storeTCdicts[stormnames[ind]] = pd.DataFrame.from_dict(create_pandadict(divdict=TCdiv_ts[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "948fc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,obj in enumerate(stormnames):\n",
    "    storeTCdicts[obj].to_csv(output+'2021_outerdiv_wpac_'+str(obj)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362633f0",
   "metadata": {},
   "source": [
    "Eq. potential temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ab4ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyear(year=None):\n",
    "    dm2 = xr.open_mfdataset([\n",
    "                             datapath+'/eqt/eqt_200_'+str(year)+'.nc',datapath+'/eqt/eqt_250_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_300_'+str(year)+'.nc',datapath+'/eqt/eqt_400_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_500_'+str(year)+'.nc',datapath+'/eqt/eqt_600_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_700_'+str(year)+'.nc',datapath+'/eqt/eqt_800_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_850_'+str(year)+'.nc',datapath+'/eqt/eqt_1000_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_925_'+str(year)+'.nc'])\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,era5_date,era5_hour\n",
    "\n",
    "dm2,era5_date,era5_hour = readyear(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4435108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67701cf1e14741d29fb4b95a444727b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCeqt_ts = []\n",
    "for TCobj in tqdm(stormnames):\n",
    "    track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "    svarname1 = ['outeqt1000','outeqt200','outeqt250','outeqt300','outeqt400',\\\n",
    "                 'outeqt500','outeqt600','outeqt700','outeqt800',\\\n",
    "                 'outeqt850','outeqt925']\n",
    "   \n",
    "    svarname = ['eqt1000','eqt200','eqt250','eqt300','eqt400',\\\n",
    "                'eqt500','eqt600','eqt700','eqt800',\\\n",
    "                'eqt850','eqt925']\n",
    "    \n",
    "    \n",
    "    svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    tsdict = {}\n",
    "    for ind,obj in (enumerate(svarname)):\n",
    "        tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "        tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "    #############################################################################################\n",
    "    TCeqt_ts.append(tsdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "97bc0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTCdicts = {}\n",
    "for ind in range(len(TCeqt_ts)):\n",
    "    storeTCdicts[stormnames[ind]] = pd.DataFrame.from_dict(TCeqt_ts[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9792b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,obj in enumerate(stormnames):\n",
    "    storeTCdicts[obj].to_csv(output+'2021_outereqt_wpac_'+str(obj)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b85c18-4367-426b-818e-6a8eac44f3ff",
   "metadata": {},
   "source": [
    "script ends here..!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08285301-ff44-4acf-b05a-099e061a8af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
